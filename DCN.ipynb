{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.jianshu.com/p/77719fc252fa\n",
    "# https://blog.csdn.net/roguesir/article/details/79777635\n",
    "# https://arxiv.org/abs/1708.05123 -Deep & Cross Network for Ad Click Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "TRAIN_FILE = r\"F:\\Data\\recsys-data\\dcn\\train.csv\"\n",
    "TEST_FILE = r\"F:\\Data\\recsys-data\\dcn\\test.csv\"\n",
    "\n",
    "SUB_DIR = \"output\"\n",
    "\n",
    "\n",
    "NUM_SPLITS = 3\n",
    "RANDOM_SEED = 2017\n",
    "\n",
    "# types of columns of the dataset dataframe\n",
    "CATEGORICAL_COLS = [\n",
    "    'ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat',\n",
    "    'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat',\n",
    "    'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat',\n",
    "    'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat',\n",
    "    'ps_car_10_cat', 'ps_car_11_cat',\n",
    "]\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    # # binary\n",
    "    # \"ps_ind_06_bin\", \"ps_ind_07_bin\", \"ps_ind_08_bin\",\n",
    "    # \"ps_ind_09_bin\", \"ps_ind_10_bin\", \"ps_ind_11_bin\",\n",
    "    # \"ps_ind_12_bin\", \"ps_ind_13_bin\", \"ps_ind_16_bin\",\n",
    "    # \"ps_ind_17_bin\", \"ps_ind_18_bin\",\n",
    "    # \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\",\n",
    "    # \"ps_calc_18_bin\", \"ps_calc_19_bin\", \"ps_calc_20_bin\",\n",
    "    # numeric\n",
    "    \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\",\n",
    "    \"ps_car_12\", \"ps_car_13\", \"ps_car_14\", \"ps_car_15\",\n",
    "\n",
    "    # feature engineering\n",
    "    \"missing_feat\", \"ps_car_13_x_ps_reg_03\",\n",
    "]\n",
    "\n",
    "IGNORE_COLS = [\n",
    "    \"id\", \"target\",\n",
    "    \"ps_calc_01\", \"ps_calc_02\", \"ps_calc_03\", \"ps_calc_04\",\n",
    "    \"ps_calc_05\", \"ps_calc_06\", \"ps_calc_07\", \"ps_calc_08\",\n",
    "    \"ps_calc_09\", \"ps_calc_10\", \"ps_calc_11\", \"ps_calc_12\",\n",
    "    \"ps_calc_13\", \"ps_calc_14\",\n",
    "    \"ps_calc_15_bin\", \"ps_calc_16_bin\", \"ps_calc_17_bin\",\n",
    "    \"ps_calc_18_bin\", \"ps_calc_19_bin\", \"ps_calc_20_bin\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDictionary(object):\n",
    "    def __init__(self,trainfile=None,testfile=None,numeric_cols = [],ignore_cols = [],cate_cols = []):\n",
    "        self.trainfile = trainfile\n",
    "        self.testfile = testfile\n",
    "        self.cate_cols = cate_cols\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.ignore_cols = ignore_cols\n",
    "        self.gen_feat_dict()\n",
    "        # feat_dict 类别字典与索引，feat_dim类别维度\n",
    "    def gen_feat_dict(self):\n",
    "        df = pd.concat([self.trainfile,self.testfile])\n",
    "        self.feat_dict = {}\n",
    "        self.feat_len = {}\n",
    "        tc = 0\n",
    "        for col in df.columns:\n",
    "            if col in self.ignore_cols or col in self.numeric_cols:\n",
    "                continue\n",
    "            else:\n",
    "                # 获取每一列的类别树\n",
    "                us = df[col].unique()\n",
    "                # 获取每一列的类别对应的维度\n",
    "                self.feat_dict[col] = dict(zip(us, range(tc, len(us) + tc)))\n",
    "                tc += len(us)\n",
    "        self.feat_dim = tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataParser(object):\n",
    "    def __init__(self,feat_dict):\n",
    "        self.feat_dict = feat_dict\n",
    "        \n",
    "    def parse(self,infile=None,df=None,has_label=False):\n",
    "        assert not ((infile is None) and (df is None))\n",
    "        assert not ((infile is not None) and (df is not None))\n",
    "        if infile is None:\n",
    "            dfi = df.copy()\n",
    "        else:\n",
    "            dfi = pd.read_csv(infile)\n",
    "        if has_label:\n",
    "            y = dfi[\"target\"].values.tolist()\n",
    "            dfi.drop(['id','target'],axis = 1,inplace=True)\n",
    "        else:\n",
    "            ids = dfi['id'].values.tolist()\n",
    "            dfi.drop(['id'],axis = 1,inplace = True)\n",
    "        # 获取数值型特征 numeric_Xv\n",
    "        numeric_Xv = dfi[self.feat_dict.numeric_cols].values.tolist()\n",
    "        dfi.drop(self.feat_dict.numeric_cols,axis = 1,inplace=True)\n",
    "        # 获取类别性特征 cate_Xi\n",
    "        dfv = dfi.copy()\n",
    "        for col in dfi.columns:\n",
    "            if col in self.feat_dict.ignore_cols:\n",
    "                dfi.drop(col,axis=1,inplace=True)\n",
    "                dfv.drop(col,axis=1,inplace=True)\n",
    "                continue\n",
    "            else:\n",
    "                #  list of list of feature indices of each sample in the dataset\n",
    "                dfi[col] = dfi[col].map(self.feat_dict.feat_dict[col])\n",
    "                # 获取除了忽略特征的其他特征值对应的值，这里的值用1.0来代替是否好？\n",
    "                dfv[col] = 1.0\n",
    "        #  list of list of feature indices of each sample in the dataset\n",
    "        cate_Xi = dfi.values.tolist()\n",
    "        # 获取除了忽略特征的其他特征值对应的值\n",
    "        cate_Xv = dfv.values.tolist()\n",
    "        if has_label:\n",
    "            return cate_Xi,cate_Xv,numeric_Xv,y\n",
    "        else:\n",
    "            return cate_Xi,cate_Xv,numeric_Xv\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCN(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, cate_feature_size,field_size, numeric_feature_size,\n",
    "                embedding_size=8,\n",
    "                deep_layers=[32, 32],dropout_deep=[0.5, 0.5, 0.5],\n",
    "                deep_layers_activation=tf.nn.relu,\n",
    "                epoch=10,batch_size=256,\n",
    "                learning_rate=0.001,optimizer_type=\"adam\",\n",
    "                batch_norm=0,batch_norm_decay=0.995,\n",
    "                verbose=False,random_seed=2016,\n",
    "                loss_type='logloss',eval_metric=roc_auc_score,\n",
    "                l2_reg=0.0,greater_is_better=True,cross_layer_num=3):\n",
    "        assert loss_type in ['logloss','mse'] # 分类用logloss，预测用mse\n",
    "        self.cate_feature_size = cate_feature_size\n",
    "        self.numeric_feature_size = numeric_feature_size\n",
    "        self.field_size = field_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.total_size = self.field_size * self.embedding_size +self.numeric_feature_size\n",
    "        self.deep_layers = deep_layers\n",
    "        self.cross_layer_num = cross_layer_num\n",
    "        self.dropout_dep = dropout_deep\n",
    "        self.deep_layers_activation =deep_layers_activation\n",
    "        self.l2_reg = l2_reg\n",
    "        \n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_type = optimizer_type\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_decay = batch_norm_decay\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.random_seed = random_seed\n",
    "        self.loss_type = loss_type\n",
    "        self.eval_metric = eval_metric\n",
    "        self.greater_is_better = greater_is_better\n",
    "        \n",
    "        self.train_result,self.valid_result = [],[]\n",
    "        self._init_graph()\n",
    "    \n",
    "    def _init_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            tf.set_random_seed(self.random_seed)\n",
    "            \n",
    "        self.feat_index = tf.placeholder(tf.int32,shape=[None,None],name='feat_index')\n",
    "        self.feat_value = tf.placeholder(tf.float32,shape=[None,None],name='feat_value')\n",
    "\n",
    "        self.numeric_value = tf.placeholder(tf.float32,shape=[None,None],name='num_value')\n",
    "\n",
    "        self.label = tf.placeholder(tf.float32,shape=[None,1],name='label')\n",
    "        self.dropout_keep_deep = tf.placeholder(tf.float32,shape=[None],name='dropout_keep_deep')\n",
    "        # 输入有五部分，feat_index是离散特征\n",
    "        self.train_phase = tf.placeholder(tf.bool,name='train_pahse')\n",
    "        # 初始化参数\n",
    "        self.weights = self._initialize_weights()\n",
    "        \n",
    "        #model\n",
    "        # 这里采用 embedding_lookup的方式处理category feature. cate_dim * embeddings_size=> field_size * emebdings_size 对应公式（1）\n",
    "        self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'],self.feat_index) # N* F* K 提取特征索引对应的参数\n",
    "        feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1]) # field_size*1\n",
    "        self.embeddings = tf.multiply(self.embeddings,feat_value)  # field_size * emebddings_size \n",
    "        # numeric_value 此处将embeddings的维度进行了延展对应公式（2）\n",
    "        self.x0 = tf.concat([self.numeric_value,tf.reshape(self.embeddings,shape=[-1,self.field_size*self.embedding_size])],axis=1)\n",
    "        \n",
    "        # deep part  \n",
    "        # dropout_keep_deep表示随机概率的列表\n",
    "        self.y_deep = tf.nn.dropout(self.x0,self.dropout_keep_deep[0])\n",
    "        # 全连接层 deep_layers代表2个全连接层，每层32维度[32,32]\n",
    "        for i in range(0,len(self.deep_layers)):\n",
    "            # 对应公式（4）\n",
    "            self.y_deep = tf.add(tf.matmul(self.y_deep,self.weights['deep_layer_%d'%i]),self.weights['deep_bias_%d'%i])\n",
    "            self.y_deep = self.deep_layers_activation(self.y_deep)\n",
    "            self.y_deep = tf.nn.dropout(self.y_deep,self.dropout_keep_deep[i+1])\n",
    "            \n",
    "        # cross part\n",
    "        # total_size表示输入的总长度\n",
    "        self._x0 = tf.reshape(self.x0, (-1,self.total_size,1))\n",
    "        x_1 = self._x0\n",
    "        \n",
    "        for l in range(self.cross_layer_num):\n",
    "            # 对应公式（3）\n",
    "            x_1 = tf.tensordot(tf.matmul(self._x0, x_1,transpose_b=True),\n",
    "                               self.weights['cross_layer_%d'%l],1) + self.weights['cross_bias_%d'%l] +x_1\n",
    "            # \n",
    "        self.cross_network_out = tf.reshape(x_1, (-1,self.total_size))\n",
    "        # concat part \n",
    "        # 对应公式（5）\n",
    "        concat_input = tf.concat([self.cross_network_out,self.y_deep],axis=1)\n",
    "        self.out = tf.add(tf.matmul(concat_input,self.weights['concat_projection']),self.weights['concat_bias'])\n",
    "        \n",
    "        # loss  \n",
    "        if self.loss_type == 'logloss':\n",
    "            self.out = tf.nn.sigmoid(self.out)\n",
    "            self.loss = tf.losses.log_loss(self.label, self.out)\n",
    "        elif self.loss_type == 'mse':\n",
    "            self.loss = tf.nn.l2_loss(tf.subtact(self.label,self.out))\n",
    "            \n",
    "        # l1 regularization on weights\n",
    "        if self.l2_reg > 0:\n",
    "            self.loss += tf.contrib.layers.l2_regularizer(\n",
    "            self.l2_reg)(self.weights['concat_projection'])\n",
    "            for i in range(len(self.deep_layers)):\n",
    "                self.loss += tf.contrib.layers.l2_regularizer(\n",
    "                self.l2_reg)(self.weights['deep_layer_%d'%i])\n",
    "            for i in range(self.cross_layer_num):\n",
    "                self.loss += tf.contrib.layers.l2_regularizer(\n",
    "                self.l2_reg)(self.weights['cross_layer_%d'%i])\n",
    "        \n",
    "        if self.optimizer_type == 'adam':\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate,beta1=0.9,beta2=0.999,epsilon=1e-8).minimize(self.loss)\n",
    "        if self.optimizer_type == 'adagrad':\n",
    "            self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate,initial_accumulator_value=1e-8).minimize(self.loss)\n",
    "        if self.optimizer_type == 'gd':\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        if self.optimizer_type == 'momentum':\n",
    "            self.optimizer = tf.train.MomentOptimizer(learning_rate=self.learning_rate,momentum=0.95).minimize(self.loss)\n",
    "        \n",
    "        # init \n",
    "        self.saver = tf.train.Saver()\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        # number of params\n",
    "        total_parameters = 0\n",
    "        for variable in self.weights.values():\n",
    "            shape = variable.get_shape()\n",
    "            print(\"shape:\",shape)\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "                variable_parameters *= dim.value\n",
    "            total_parameters += variable_parameters\n",
    "        if self.verbose > 0:\n",
    "            print(\"#params:%d\" % total_parameters)\n",
    "    def _initialize_weights(self):\n",
    "        weights = dict()\n",
    "        \n",
    "        # embenddings cate_feature_size为category词典个数，embeddingsize是embedding后的维度\n",
    "        weights['feature_embeddings'] = tf.Variable(\n",
    "        tf.random_normal([self.cate_feature_size,self.embedding_size],0.0,0.01),name='feature_embeddings')\n",
    "        weights['feature_bias'] = tf.Variable(tf.random_normal([self.cate_feature_size,1],0.0,0.01,name='feature_bias'))\n",
    "        \n",
    "        # deep layers\n",
    "        num_layer = len(self.deep_layers)\n",
    "        glorot = np.sqrt(2.0/(self.total_size + self.deep_layers[0]))\n",
    "        \n",
    "        weights['deep_layer_0'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(self.total_size,self.deep_layers[0])),dtype=np.float32)\n",
    "        \n",
    "        weights['deep_bias_0'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(1,self.deep_layers[0])),dtype=np.float32)\n",
    "        \n",
    "        for i in range(1,num_layer):\n",
    "            glorot = np.sqrt(2.0/(self.deep_layers[i-1] + self.deep_layers[i]))\n",
    "            # layers[i-1] * layers[i]\n",
    "            weights['deep_layer_%d' % i] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(self.deep_layers[i-1],self.deep_layers[i])),dtype=np.float32)\n",
    "            # 1* layer[i]\n",
    "            weights['deep_bias_%d' % i] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(1,self.deep_layers[i])),dtype=np.float32)\n",
    "        \n",
    "        # cross layers\n",
    "        for i in range(self.cross_layer_num):\n",
    "            weights['cross_layer_%d' % i] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(self.total_size,1)),dtype=np.float32)\n",
    "            # 1* layer[i]\n",
    "            weights['cross_bias_%d' % i] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(self.total_size,1)),dtype=np.float32)\n",
    "        \n",
    "        # final concat projection layer\n",
    "        input_size = self.total_size + self.deep_layers[-1]\n",
    "        \n",
    "        glorot = np.sqrt(2.0/(input_size + 1))\n",
    "        weights['concat_projection'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(input_size,1)),dtype=np.float32)\n",
    "        weights['concat_bias'] = tf.Variable(tf.constant(0.01),dtype=np.float32)\n",
    "        \n",
    "        return weights\n",
    "    # 获取batch的数据\n",
    "    def get_batch(self,Xi,Xv,Xv2,y,batch_size,index):\n",
    "        start = index * batch_size\n",
    "        end = (index+1)* batch_size\n",
    "        end =end if end < len(y) else len(y)\n",
    "        return Xi[start:end],Xv[start:end],Xv2[start:end],[[y_] for y_ in y[start:end]]\n",
    "    \n",
    "    # shuffle three lists simutaneously\n",
    "    def shuffle_in_unison_scary(self,a,b,c,d):\n",
    "        rng_state = np.random.get_state()\n",
    "        np.random.shuffle(a)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(b)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(c)\n",
    "        np.random.set_state(rng_state)\n",
    "        np.random.shuffle(d)\n",
    "        np.random.set_state(rng_state)\n",
    "    \n",
    "    def predict(self,Xi,Xv,Xv2,y):\n",
    "        \"\"\"\n",
    "        :param Xi: list of list of feature indices of each sample in the dataset\n",
    "        :param Xv: list of list of feature values of each sample in the dataset\n",
    "        :return: predicted probability of each sample\n",
    "        \"\"\"\n",
    "        \n",
    "        feed_dict = {self.feat_index: Xi,\n",
    "                     self.feat_value: Xv,\n",
    "                     self.numeric_value: Xv2,\n",
    "                     self.label: y,\n",
    "                     self.dropout_keep_deep: [1.0] * len(self.dropout_dep),\n",
    "                     self.train_phase: True}\n",
    "        loss = self.sess.run([self.loss],feed_dict=feed_dict)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # 在每个batch上训练\n",
    "    def fit_on_batch(self,Xi,Xv,Xv2,y):\n",
    "        feed_dict = {self.feat_index:Xi,\n",
    "                     self.feat_value:Xv,\n",
    "                     self.numeric_value:Xv2,\n",
    "                     self.label:y,\n",
    "                     self.dropout_keep_deep:self.dropout_dep,\n",
    "                     self.train_phase:True}\n",
    "\n",
    "        loss,opt = self.sess.run([self.loss,self.optimizer],feed_dict=feed_dict)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def fit(self,cate_Xi_train,cate_Xv_train,numeric_Xv_train,y_train,cate_Xi_valid=None,cate_Xv_valid=None,numeric_Xv_valid=None,y_valid=None,early_stopping=False,refit=False):\n",
    "        \"\"\"\n",
    "        :param Xi_train: [[ind1_1, ind1_2, ...], [ind2_1, ind2_2, ...], ..., [indi_1, indi_2, ..., indi_j, ...], ...]\n",
    "                         indi_j is the feature index of feature field j of sample i in the training set\n",
    "        :param Xv_train: [[val1_1, val1_2, ...], [val2_1, val2_2, ...], ..., [vali_1, vali_2, ..., vali_j, ...], ...]\n",
    "                         vali_j is the feature value of feature field j of sample i in the training set\n",
    "                         vali_j can be either binary (1/0, for binary/categorical features) or float (e.g., 10.24, for numerical features)\n",
    "        :param y_train: label of each sample in the training set\n",
    "        :param Xi_valid: list of list of feature indices of each sample in the validation set\n",
    "        :param Xv_valid: list of list of feature values of each sample in the validation set\n",
    "        :param y_valid: label of each sample in the validation set\n",
    "        :param early_stopping: perform early stopping or not\n",
    "        :param refit: refit the model on the train+valid dataset or not\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(len(cate_Xi_train))\n",
    "        print(len(cate_Xv_train))\n",
    "        print(len(numeric_Xv_train))\n",
    "        print(len(y_train))\n",
    "        \n",
    "        has_valid = cate_Xv_valid is not None\n",
    "        for epoch in range(self.epoch):\n",
    "            t1 = time()\n",
    "            self.shuffle_in_unison_scary(cate_Xi_train,cate_Xv_train,numeric_Xv_train,y_train)\n",
    "            total_batch = int(len(y_train)/self.batch_size)\n",
    "            for i in range(total_batch):\n",
    "                cate_Xi_batch, cate_Xv_batch,numeric_Xv_batch, y_batch = self.get_batch(cate_Xi_train, cate_Xv_train, numeric_Xv_train,y_train, self.batch_size, i)\n",
    "                \n",
    "                self.fit_on_batch(cate_Xi_batch, cate_Xv_batch,numeric_Xv_batch, y_batch)\n",
    "                \n",
    "            if has_valid:\n",
    "                y_valid = np.array(y_valid).reshape((-1,1))\n",
    "                loss = self.predict(cate_Xi_valid, cate_Xv_valid, numeric_Xv_valid, y_valid)\n",
    "                print(\"epoch\",epoch,\"loss\",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dfTrain = pd.read_csv(TRAIN_FILE)\n",
    "    dfTest = pd.read_csv(TEST_FILE)\n",
    "    \n",
    "    def preprocess(df):\n",
    "        cols = [c for c in df.columns if c not in ['id','target']]\n",
    "        df['missing_feat'] = np.sum((df[cols]==-1).values,axis=1)\n",
    "        df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "        return df\n",
    "    dfTrain = preprocess(dfTrain)\n",
    "    dfTest = preprocess(dfTest)\n",
    "    \n",
    "    cols = [c for c in dfTrain.columns if c not in ['id','target']]\n",
    "    cols = [c for c in cols if (not c in IGNORE_COLS)]\n",
    "    \n",
    "    X_train = dfTrain[cols].values\n",
    "    y_train = dfTrain['target'].values\n",
    "    \n",
    "    X_test = dfTest[cols].values\n",
    "    ids_test = dfTest['id'].values\n",
    "    \n",
    "    return dfTrain, dfTest,X_train,y_train,X_test,ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain, dfTest,X_train,y_train,X_test,ids_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'target', 'ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03',\n",
       "       'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin',\n",
       "       'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin',\n",
       "       'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15',\n",
       "       'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01',\n",
       "       'ps_reg_02', 'ps_reg_03', 'ps_car_01_cat', 'ps_car_02_cat',\n",
       "       'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat',\n",
       "       'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_10_cat',\n",
       "       'ps_car_11_cat', 'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14',\n",
       "       'ps_car_15', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04',\n",
       "       'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09',\n",
       "       'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14',\n",
       "       'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin',\n",
       "       'ps_calc_19_bin', 'ps_calc_20_bin', 'missing_feat',\n",
       "       'ps_car_13_x_ps_reg_03'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 39)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\julianxu\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "fd = FeatureDictionary(dfTrain,dfTest,numeric_cols = NUMERIC_COLS,ignore_cols=IGNORE_COLS,cate_cols=CATEGORICAL_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parser = DataParser(feat_dict=fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ps_car_01_cat': {10: 0,\n",
       "  11: 1,\n",
       "  7: 2,\n",
       "  6: 3,\n",
       "  9: 4,\n",
       "  5: 5,\n",
       "  4: 6,\n",
       "  8: 7,\n",
       "  3: 8,\n",
       "  0: 9,\n",
       "  2: 10,\n",
       "  1: 11,\n",
       "  -1: 12},\n",
       " 'ps_car_02_cat': {1: 13, 0: 14},\n",
       " 'ps_car_03_cat': {-1: 15, 0: 16, 1: 17},\n",
       " 'ps_car_04_cat': {0: 18,\n",
       "  1: 19,\n",
       "  8: 20,\n",
       "  9: 21,\n",
       "  2: 22,\n",
       "  6: 23,\n",
       "  3: 24,\n",
       "  7: 25,\n",
       "  4: 26,\n",
       "  5: 27},\n",
       " 'ps_car_05_cat': {1: 28, -1: 29, 0: 30},\n",
       " 'ps_car_06_cat': {4: 31,\n",
       "  11: 32,\n",
       "  14: 33,\n",
       "  13: 34,\n",
       "  6: 35,\n",
       "  15: 36,\n",
       "  3: 37,\n",
       "  0: 38,\n",
       "  1: 39,\n",
       "  10: 40,\n",
       "  12: 41,\n",
       "  9: 42,\n",
       "  17: 43,\n",
       "  7: 44,\n",
       "  8: 45,\n",
       "  5: 46,\n",
       "  2: 47,\n",
       "  16: 48},\n",
       " 'ps_car_07_cat': {1: 49, -1: 50, 0: 51},\n",
       " 'ps_car_08_cat': {0: 52, 1: 53},\n",
       " 'ps_car_09_cat': {0: 54, 2: 55, 3: 56, 1: 57, -1: 58, 4: 59},\n",
       " 'ps_car_10_cat': {1: 60, 0: 61, 2: 62},\n",
       " 'ps_car_11': {2: 63, 3: 64, 1: 65, 0: 66},\n",
       " 'ps_car_11_cat': {12: 67,\n",
       "  19: 68,\n",
       "  60: 69,\n",
       "  104: 70,\n",
       "  82: 71,\n",
       "  99: 72,\n",
       "  30: 73,\n",
       "  68: 74,\n",
       "  20: 75,\n",
       "  36: 76,\n",
       "  101: 77,\n",
       "  103: 78,\n",
       "  41: 79,\n",
       "  59: 80,\n",
       "  43: 81,\n",
       "  64: 82,\n",
       "  29: 83,\n",
       "  95: 84,\n",
       "  24: 85,\n",
       "  5: 86,\n",
       "  28: 87,\n",
       "  87: 88,\n",
       "  66: 89,\n",
       "  10: 90,\n",
       "  26: 91,\n",
       "  54: 92,\n",
       "  32: 93,\n",
       "  38: 94,\n",
       "  83: 95,\n",
       "  89: 96,\n",
       "  49: 97,\n",
       "  93: 98,\n",
       "  1: 99,\n",
       "  22: 100,\n",
       "  85: 101,\n",
       "  78: 102,\n",
       "  31: 103,\n",
       "  34: 104,\n",
       "  7: 105,\n",
       "  8: 106,\n",
       "  3: 107,\n",
       "  46: 108,\n",
       "  27: 109,\n",
       "  25: 110,\n",
       "  61: 111,\n",
       "  16: 112,\n",
       "  69: 113,\n",
       "  40: 114,\n",
       "  76: 115,\n",
       "  39: 116,\n",
       "  88: 117,\n",
       "  42: 118,\n",
       "  75: 119,\n",
       "  91: 120,\n",
       "  23: 121,\n",
       "  2: 122,\n",
       "  71: 123,\n",
       "  90: 124,\n",
       "  80: 125,\n",
       "  44: 126,\n",
       "  92: 127,\n",
       "  72: 128,\n",
       "  96: 129,\n",
       "  86: 130,\n",
       "  62: 131,\n",
       "  33: 132,\n",
       "  67: 133,\n",
       "  73: 134,\n",
       "  77: 135,\n",
       "  18: 136,\n",
       "  21: 137,\n",
       "  74: 138,\n",
       "  37: 139,\n",
       "  48: 140,\n",
       "  70: 141,\n",
       "  13: 142,\n",
       "  15: 143,\n",
       "  102: 144,\n",
       "  53: 145,\n",
       "  65: 146,\n",
       "  100: 147,\n",
       "  51: 148,\n",
       "  79: 149,\n",
       "  52: 150,\n",
       "  63: 151,\n",
       "  94: 152,\n",
       "  6: 153,\n",
       "  57: 154,\n",
       "  35: 155,\n",
       "  98: 156,\n",
       "  56: 157,\n",
       "  97: 158,\n",
       "  55: 159,\n",
       "  84: 160,\n",
       "  50: 161,\n",
       "  4: 162,\n",
       "  58: 163,\n",
       "  9: 164,\n",
       "  17: 165,\n",
       "  11: 166,\n",
       "  45: 167,\n",
       "  14: 168,\n",
       "  81: 169,\n",
       "  47: 170},\n",
       " 'ps_car_14_x_ps_reg_03': {0.6345436128256319: 171,\n",
       "  0.4740615185329165: 172,\n",
       "  -0.6415857163: 173,\n",
       "  0.315424743152033: 174,\n",
       "  0.4757276939561555: 175,\n",
       "  2.0505126853690014: 176,\n",
       "  0.3949750554567522: 177,\n",
       "  0.5468715024014198: 178,\n",
       "  0.7036605513588805: 179,\n",
       "  3.3781052652254715: 180,\n",
       "  0.6963874182984052: 181,\n",
       "  0.3611018742292608: 182,\n",
       "  0.41962987306228655: 183,\n",
       "  0.6727781576290176: 184,\n",
       "  0.5965348025713246: 185,\n",
       "  -1.7191579719: 186,\n",
       "  0.5342517528504355: 187,\n",
       "  0.7105556695114986: 188,\n",
       "  -0.6682281988000001: 189,\n",
       "  0.18692311391584768: 190,\n",
       "  0.780780809485334: 191,\n",
       "  0.5505663875843462: 192,\n",
       "  0.730490970791648: 193,\n",
       "  -0.7287343268: 194,\n",
       "  0.4235142966999767: 195,\n",
       "  -0.8947505308: 196,\n",
       "  0.8757120036489767: 197,\n",
       "  0.7035049501289431: 198,\n",
       "  -0.720264529: 199,\n",
       "  0.7424985217401213: 200,\n",
       "  0.7007052877352381: 201,\n",
       "  0.6141355713484652: 202,\n",
       "  0.6058024568876295: 203,\n",
       "  0.39293538650913423: 204,\n",
       "  0.7565458747531519: 205,\n",
       "  0.810139450760228: 206,\n",
       "  0.5773189852691626: 207,\n",
       "  0.46286303319196104: 208,\n",
       "  0.6031915482994595: 209,\n",
       "  0.6816452152741763: 210,\n",
       "  0.453797168132806: 211,\n",
       "  0.831818927043296: 212,\n",
       "  0.4339228761623763: 213,\n",
       "  1.3450623445096193: 214,\n",
       "  0.9628874126819614: 215,\n",
       "  0.7846444028598041: 216,\n",
       "  0.5259320607144944: 217,\n",
       "  -0.9707818268999999: 218,\n",
       "  0.383642153132922: 219,\n",
       "  1.3019117181025788: 220,\n",
       "  1.4782944674512464: 221,\n",
       "  1.1280310304640742: 222,\n",
       "  0.48056018769051045: 223,\n",
       "  0.37527882195465306: 224,\n",
       "  0.45087959262182314: 225,\n",
       "  -0.7492282531000001: 226,\n",
       "  0.4676099772875808: 227,\n",
       "  0.9578426515962368: 228,\n",
       "  0.3917698636293656: 229,\n",
       "  0.35224625999965103: 230,\n",
       "  -0.7872072459: 231,\n",
       "  0.27134781341782616: 232,\n",
       "  0.5262670660438891: 233,\n",
       "  0.580106680138798: 234,\n",
       "  0.6716908478327193: 235,\n",
       "  0.48085734497042126: 236,\n",
       "  -1.2409215784: 237,\n",
       "  0.4141972657707631: 238,\n",
       "  1.1359298660260992: 239,\n",
       "  0.9457912970222804: 240,\n",
       "  -0.7014006059000001: 241,\n",
       "  0.4003759209880368: 242,\n",
       "  0.3148644639915831: 243,\n",
       "  -0.3773001085: 244,\n",
       "  1.0302191504547524: 245,\n",
       "  0.6203975997329951: 246,\n",
       "  0.6489747257747782: 247,\n",
       "  0.8046142111781298: 248,\n",
       "  0.9830192766348057: 249,\n",
       "  0.4751291962545778: 250,\n",
       "  -1.2653030212: 251,\n",
       "  0.5323587724463483: 252,\n",
       "  -0.8232168307: 253,\n",
       "  0.28933603458720775: 254,\n",
       "  0.592510042852663: 255,\n",
       "  0.48878169086977125: 256,\n",
       "  0.631850882118936: 257,\n",
       "  0.4096714315163262: 258,\n",
       "  1.3986445174034055: 259,\n",
       "  -0.5004956221: 260,\n",
       "  0.8935324834153687: 261,\n",
       "  0.4288196278423098: 262,\n",
       "  -0.5013205701: 263,\n",
       "  1.3249964524844584: 264,\n",
       "  0.5008280849211573: 265,\n",
       "  1.5199562558233841: 266,\n",
       "  0.45909621117797544: 267,\n",
       "  0.5716272806085625: 268,\n",
       "  0.8885434008178642: 269,\n",
       "  0.4823903120720279: 270,\n",
       "  0.46574539803997167: 271,\n",
       "  0.4639501500145087: 272,\n",
       "  1.4479936871723926: 273,\n",
       "  -0.6419237616: 274,\n",
       "  0.726197287224339: 275,\n",
       "  -1.1109150424: 276,\n",
       "  0.5844551393635121: 277,\n",
       "  0.477993976168972: 278,\n",
       "  0.5506304496039045: 279,\n",
       "  -0.9463937643000001: 280,\n",
       "  -0.5310600657: 281,\n",
       "  0.8082295992963018: 282,\n",
       "  1.722782987347144: 283,\n",
       "  0.9079277208999746: 284,\n",
       "  0.6371110870378491: 285,\n",
       "  0.7438485853945676: 286,\n",
       "  1.1369914516275512: 287,\n",
       "  0.4461801835720269: 288,\n",
       "  0.4450445121392793: 289,\n",
       "  0.5801725662507692: 290,\n",
       "  1.2173979028867004: 291,\n",
       "  -0.6017021311: 292,\n",
       "  -0.8203884385: 293,\n",
       "  0.7263675119351652: 294,\n",
       "  0.4380516003547291: 295,\n",
       "  0.6159986097568633: 296,\n",
       "  0.7227850802772223: 297,\n",
       "  1.20800154916637: 298,\n",
       "  0.4634022776863885: 299,\n",
       "  0.44668579833883476: 300,\n",
       "  0.6256435529543319: 301,\n",
       "  -0.830388737: 302,\n",
       "  0.5738315921993209: 303,\n",
       "  0.6632198256890239: 304,\n",
       "  -0.9963466322: 305,\n",
       "  0.5514759380865036: 306,\n",
       "  0.6158736047749702: 307,\n",
       "  1.3129282556189077: 308,\n",
       "  0.4771393752765494: 309,\n",
       "  0.3934874666196917: 310,\n",
       "  0.7286407902068787: 311,\n",
       "  1.7987186765631036: 312,\n",
       "  1.0988340810320474: 313,\n",
       "  0.6525874774942837: 314,\n",
       "  0.5124420447047701: 315,\n",
       "  -0.7189006759000001: 316,\n",
       "  -1.0866488785: 317,\n",
       "  0.8712077280347011: 318,\n",
       "  0.67697858162: 319,\n",
       "  0.6052545610770792: 320,\n",
       "  0.5828672183660775: 321,\n",
       "  1.0174684556388889: 322,\n",
       "  0.45410074833446684: 323,\n",
       "  0.6730257803213066: 324,\n",
       "  -0.6587146261: 325,\n",
       "  0.22933113306796285: 326,\n",
       "  2.1906972502171356: 327,\n",
       "  1.1682727237500001: 328,\n",
       "  -1.5541107845: 329,\n",
       "  0.4873311584972618: 330,\n",
       "  0.7771377308282426: 331,\n",
       "  1.8098460050495415: 332,\n",
       "  1.3802514340981782: 333,\n",
       "  0.3633287050535108: 334,\n",
       "  0.3829016013938222: 335,\n",
       "  0.4901272182534663: 336,\n",
       "  -0.7887771684999999: 337,\n",
       "  0.6451833417078227: 338,\n",
       "  0.5025868310173541: 339,\n",
       "  1.2735618672730933: 340,\n",
       "  0.4858265673564261: 341,\n",
       "  0.6792712182504826: 342,\n",
       "  0.3683925813607666: 343,\n",
       "  0.7937253932546959: 344,\n",
       "  0.6842100753878465: 345,\n",
       "  0.6259384075376784: 346,\n",
       "  0.6321304513670876: 347,\n",
       "  0.531263259429877: 348,\n",
       "  -0.881862698: 349,\n",
       "  0.7067840809232497: 350,\n",
       "  -1.076860297: 351,\n",
       "  0.581540518835368: 352,\n",
       "  0.49223011175394726: 353,\n",
       "  0.8499546126173053: 354,\n",
       "  0.2893644851717949: 355,\n",
       "  0.36591964270242966: 356,\n",
       "  0.6709471321488009: 357,\n",
       "  0.5302721311072839: 358,\n",
       "  0.4892750318635779: 359,\n",
       "  0.49514732973374: 360,\n",
       "  0.5754066227600294: 361,\n",
       "  1.032829425307867: 362,\n",
       "  -0.7683835131: 363,\n",
       "  0.9357685250198038: 364,\n",
       "  0.4331279582742447: 365,\n",
       "  0.4820912069346089: 366,\n",
       "  0.9443343862071086: 367,\n",
       "  0.32643170424704065: 368,\n",
       "  0.7967646678732011: 369,\n",
       "  0.7904116773847989: 370,\n",
       "  0.9687366567982595: 371,\n",
       "  0.46764742224610495: 372,\n",
       "  0.5208004642683879: 373,\n",
       "  0.354700577983482: 374,\n",
       "  0.6227961143195511: 375,\n",
       "  0.5639511584118951: 376,\n",
       "  0.6808678532722754: 377,\n",
       "  0.6648308055619276: 378,\n",
       "  0.785682469065: 379,\n",
       "  1.4259268951583166: 380,\n",
       "  0.48662113508523935: 381,\n",
       "  0.41103446145059624: 382,\n",
       "  -0.5050775245: 383,\n",
       "  0.6975945609932993: 384,\n",
       "  0.8345649661934407: 385,\n",
       "  0.5251671074204532: 386,\n",
       "  -0.8156399911: 387,\n",
       "  0.3004866275326368: 388,\n",
       "  0.42707374175298357: 389,\n",
       "  1.2046493093262283: 390,\n",
       "  0.5894932558009487: 391,\n",
       "  -0.910929958: 392,\n",
       "  -1.2377916219: 393,\n",
       "  2.587148574373599: 394,\n",
       "  0.7893429177030936: 395,\n",
       "  2.4551431563775097: 396,\n",
       "  0.2958220179939973: 397,\n",
       "  0.3746579707517969: 398,\n",
       "  -0.9164249619: 399,\n",
       "  0.4471742951616994: 400,\n",
       "  -0.8023210131999999: 401,\n",
       "  0.6154371095037282: 402,\n",
       "  2.4474659268366192: 403,\n",
       "  0.48422869358640747: 404,\n",
       "  0.8282765980914327: 405,\n",
       "  0.6126586276682141: 406,\n",
       "  0.5906556930520408: 407,\n",
       "  0.7443075814206401: 408,\n",
       "  0.38334206180422853: 409,\n",
       "  0.4000980316584289: 410,\n",
       "  0.8267827673351218: 411,\n",
       "  0.6467438906327528: 412,\n",
       "  0.8245731662768703: 413,\n",
       "  0.5807702565508358: 414,\n",
       "  0.5639401673951567: 415,\n",
       "  0.46936792607951855: 416,\n",
       "  0.7951910765128427: 417,\n",
       "  0.7419971791963397: 418,\n",
       "  0.8199231155208648: 419,\n",
       "  0.5047634663694748: 420,\n",
       "  0.7144388311245218: 421,\n",
       "  0.9208824984389242: 422,\n",
       "  0.3320203696194292: 423,\n",
       "  0.6690858043043565: 424,\n",
       "  0.8464665863049026: 425,\n",
       "  0.5628402895819121: 426,\n",
       "  0.8461200642879408: 427,\n",
       "  0.586571492397814: 428,\n",
       "  0.8767911419600414: 429,\n",
       "  1.124711825984632: 430,\n",
       "  0.8224806103627229: 431,\n",
       "  0.5645137850139474: 432,\n",
       "  0.7955225376902938: 433,\n",
       "  -1.0814265907: 434,\n",
       "  0.5632409146498968: 435,\n",
       "  0.420051326919263: 436,\n",
       "  0.9100652094261533: 437,\n",
       "  0.7615027009797507: 438,\n",
       "  0.4331744251718733: 439,\n",
       "  -0.9795639104: 440,\n",
       "  0.460612913605874: 441,\n",
       "  0.5793896515249243: 442,\n",
       "  0.4986345508678548: 443,\n",
       "  0.6420421962307746: 444,\n",
       "  0.7205932194907749: 445,\n",
       "  0.46443294286192394: 446,\n",
       "  0.478583454532739: 447,\n",
       "  0.6313939468661237: 448,\n",
       "  0.44515583095967826: 449,\n",
       "  0.352116717622343: 450,\n",
       "  0.47791569241209886: 451,\n",
       "  0.44515736839644765: 452,\n",
       "  0.6415675051689828: 453,\n",
       "  0.3647863562754167: 454,\n",
       "  0.3277039062905773: 455,\n",
       "  1.3532349135683606: 456,\n",
       "  0.7720530481005283: 457,\n",
       "  0.4748487144481541: 458,\n",
       "  0.9697053206937386: 459,\n",
       "  -0.7916141058: 460,\n",
       "  2.051837933987706: 461,\n",
       "  0.7407140795456761: 462,\n",
       "  0.6924943387094596: 463,\n",
       "  0.8900919040599012: 464,\n",
       "  1.443391793079799: 465,\n",
       "  0.9315478612835537: 466,\n",
       "  0.5869225005703319: 467,\n",
       "  0.5734654176322608: 468,\n",
       "  0.3374767554134596: 469,\n",
       "  -0.8071606188: 470,\n",
       "  -0.7723394749: 471,\n",
       "  0.5540314426154374: 472,\n",
       "  0.6518630227213807: 473,\n",
       "  0.3396700969712638: 474,\n",
       "  -0.7328620449: 475,\n",
       "  1.1480639358190188: 476,\n",
       "  0.5139063801157375: 477,\n",
       "  0.7290891555985427: 478,\n",
       "  0.6756227995499612: 479,\n",
       "  0.887089504662503: 480,\n",
       "  1.0424670939956426: 481,\n",
       "  0.4410492835525795: 482,\n",
       "  -0.5995349436999999: 483,\n",
       "  0.8748884374360484: 484,\n",
       "  0.605370176116312: 485,\n",
       "  1.0404633511056076: 486,\n",
       "  1.4420206137846896: 487,\n",
       "  0.23342458122792187: 488,\n",
       "  1.3792869871244433: 489,\n",
       "  2.0514105835027414: 490,\n",
       "  -0.5841572301: 491,\n",
       "  0.45617520328263467: 492,\n",
       "  0.7553746983531054: 493,\n",
       "  0.3782561115761246: 494,\n",
       "  1.1729171578712059: 495,\n",
       "  -0.9624778628: 496,\n",
       "  0.424110091653822: 497,\n",
       "  0.9284742661500608: 498,\n",
       "  0.41826407140630995: 499,\n",
       "  0.5067374961154034: 500,\n",
       "  0.7621119075741248: 501,\n",
       "  -0.7016656687999999: 502,\n",
       "  0.9648489017618191: 503,\n",
       "  0.5344517397613634: 504,\n",
       "  0.23697582435887515: 505,\n",
       "  0.6718492908051718: 506,\n",
       "  0.43493905121905: 507,\n",
       "  0.44719020625029304: 508,\n",
       "  0.3397772878707546: 509,\n",
       "  1.5176164400437349: 510,\n",
       "  0.5650167869861649: 511,\n",
       "  1.5160857110091106: 512,\n",
       "  0.43056750705820823: 513,\n",
       "  0.9413527239749999: 514,\n",
       "  0.3009906165516189: 515,\n",
       "  0.3194739388986616: 516,\n",
       "  1.8503091760245574: 517,\n",
       "  0.5445465928095266: 518,\n",
       "  0.5004378248421716: 519,\n",
       "  0.4012957447627523: 520,\n",
       "  0.5056416097327717: 521,\n",
       "  0.7009277997851999: 522,\n",
       "  -0.7858248284999999: 523,\n",
       "  0.3772471411294675: 524,\n",
       "  0.6687009617696937: 525,\n",
       "  0.9473101396213828: 526,\n",
       "  1.570755792700902: 527,\n",
       "  1.3703867011637567: 528,\n",
       "  0.840745407912427: 529,\n",
       "  1.4278410945209714: 530,\n",
       "  2.75996304273797: 531,\n",
       "  0.553678299296438: 532,\n",
       "  -0.7866657328: 533,\n",
       "  -0.5242468033: 534,\n",
       "  0.3488398903542491: 535,\n",
       "  1.2649221341208128: 536,\n",
       "  1.6887329517087817: 537,\n",
       "  0.7990939472044851: 538,\n",
       "  1.1762498338952536: 539,\n",
       "  1.0006844249105205: 540,\n",
       "  -0.441985565: 541,\n",
       "  0.5370076064807819: 542,\n",
       "  -0.8733780717: 543,\n",
       "  0.8095750434403591: 544,\n",
       "  0.986535784397166: 545,\n",
       "  0.6021516602281937: 546,\n",
       "  -0.5896139653: 547,\n",
       "  0.5076589540939119: 548,\n",
       "  0.6552946869165345: 549,\n",
       "  0.5228288932964428: 550,\n",
       "  0.7205640689254982: 551,\n",
       "  -0.6938335237000001: 552,\n",
       "  0.8153325625974455: 553,\n",
       "  -0.8901782717: 554,\n",
       "  0.6349771930612798: 555,\n",
       "  -0.6123218243: 556,\n",
       "  0.49709473612044835: 557,\n",
       "  0.6677709776448901: 558,\n",
       "  -0.8769900628: 559,\n",
       "  0.5432057573735022: 560,\n",
       "  -1.4882004913999998: 561,\n",
       "  0.7388722525440599: 562,\n",
       "  0.2720343707356485: 563,\n",
       "  -1.1721192927: 564,\n",
       "  -1.4304972262: 565,\n",
       "  1.0697499707930094: 566,\n",
       "  0.8630385340631292: 567,\n",
       "  0.712272277685: 568,\n",
       "  1.4714703531000002: 569,\n",
       "  0.9220130745243807: 570,\n",
       "  0.32038422024373947: 571,\n",
       "  0.36727835279421084: 572,\n",
       "  0.5516688266753904: 573,\n",
       "  -0.8028358827: 574,\n",
       "  0.8480251540602739: 575,\n",
       "  1.3243104006673188: 576,\n",
       "  1.8532488364406507: 577,\n",
       "  -0.936407766: 578,\n",
       "  0.8639822871066078: 579,\n",
       "  0.7844537039269793: 580,\n",
       "  0.4715990141841089: 581,\n",
       "  2.7605143896337774: 582,\n",
       "  0.44667781948142354: 583,\n",
       "  0.6677039010618163: 584,\n",
       "  0.40146016094307324: 585,\n",
       "  0.2874392677940801: 586,\n",
       "  0.3898660105253475: 587,\n",
       "  0.865037440117659: 588,\n",
       "  0.5877405806623898: 589,\n",
       "  1.3462207488676579: 590,\n",
       "  1.1021092895965783: 591,\n",
       "  2.5789057177041794: 592,\n",
       "  0.9333429321827886: 593,\n",
       "  0.976177610499809: 594,\n",
       "  1.0428518863081728: 595,\n",
       "  0.5349425270324939: 596,\n",
       "  1.5019284917458402: 597,\n",
       "  0.75330975972775: 598,\n",
       "  0.4817703664702628: 599,\n",
       "  0.28022653541853554: 600,\n",
       "  -0.6057063359: 601,\n",
       "  0.3603431668039597: 602,\n",
       "  0.6331843470738565: 603,\n",
       "  0.526319916893359: 604,\n",
       "  1.5061949169083646: 605,\n",
       "  0.7457057416691618: 606,\n",
       "  0.9099436866591942: 607,\n",
       "  0.34201849788112115: 608,\n",
       "  -0.7531812695000001: 609,\n",
       "  1.8406796994385348: 610,\n",
       "  0.4952342087725921: 611,\n",
       "  0.7068628676709023: 612,\n",
       "  0.6280451724076129: 613,\n",
       "  1.2935309282548475: 614,\n",
       "  0.44227039151169867: 615,\n",
       "  0.3623779717947517: 616,\n",
       "  0.6478698706987285: 617,\n",
       "  -0.6671451451: 618,\n",
       "  0.687442390499076: 619,\n",
       "  0.39188989006585245: 620,\n",
       "  0.7883387406168697: 621,\n",
       "  1.0158603431259108: 622,\n",
       "  -0.9040885095000001: 623,\n",
       "  0.4400960022517106: 624,\n",
       "  0.5675496482949838: 625,\n",
       "  -0.5539117094: 626,\n",
       "  0.6072155874322821: 627,\n",
       "  1.3123473937150716: 628,\n",
       "  1.0635072886396881: 629,\n",
       "  0.4490264376839114: 630,\n",
       "  1.7140225632461976: 631,\n",
       "  0.8752806367916928: 632,\n",
       "  -0.7071652173: 633,\n",
       "  1.4187444933954867: 634,\n",
       "  0.5774063799228115: 635,\n",
       "  0.7335646860955366: 636,\n",
       "  0.3494772697697578: 637,\n",
       "  0.5249134619765918: 638,\n",
       "  0.6557132722610775: 639,\n",
       "  -1.0365251070000001: 640,\n",
       "  -0.7368404741: 641,\n",
       "  -0.9314966186: 642,\n",
       "  1.7158954484473303: 643,\n",
       "  2.928552180103879: 644,\n",
       "  -0.9567134471: 645,\n",
       "  0.569039670549732: 646,\n",
       "  -0.7747700272: 647,\n",
       "  -0.9104648707999999: 648,\n",
       "  0.8283901045953022: 649,\n",
       "  0.804223741500447: 650,\n",
       "  -0.6667269257: 651,\n",
       "  0.5024516598106787: 652,\n",
       "  0.7490603617599108: 653,\n",
       "  1.3179924922280184: 654,\n",
       "  0.4159971032405735: 655,\n",
       "  0.7781817850414167: 656,\n",
       "  0.33739533887319373: 657,\n",
       "  0.585791045772015: 658,\n",
       "  0.5832036062115576: 659,\n",
       "  0.7094408008672762: 660,\n",
       "  -0.6983007711: 661,\n",
       "  0.46384754547464446: 662,\n",
       "  0.9626930459507946: 663,\n",
       "  0.6695024056185044: 664,\n",
       "  0.8758619942658361: 665,\n",
       "  0.7409126114868065: 666,\n",
       "  0.451790514977887: 667,\n",
       "  0.5195454545299661: 668,\n",
       "  0.6031463218547504: 669,\n",
       "  1.2089849577255753: 670,\n",
       "  0.532268360338665: 671,\n",
       "  -0.7595262769: 672,\n",
       "  0.7000357945513197: 673,\n",
       "  0.7784592219702604: 674,\n",
       "  -0.7140271078: 675,\n",
       "  0.80057692424445: 676,\n",
       "  0.5771469306477525: 677,\n",
       "  0.5311233806172337: 678,\n",
       "  0.42770938455015506: 679,\n",
       "  -0.6526012793: 680,\n",
       "  -0.8443174171: 681,\n",
       "  0.5696470199111419: 682,\n",
       "  1.4747875949419986: 683,\n",
       "  2.8279250562485068: 684,\n",
       "  0.7632453679785097: 685,\n",
       "  0.49960624994: 686,\n",
       "  0.7516881311389751: 687,\n",
       "  0.829514009365739: 688,\n",
       "  0.7927879329694371: 689,\n",
       "  0.41546045102385554: 690,\n",
       "  0.4363352263320993: 691,\n",
       "  1.06540454350522: 692,\n",
       "  -0.8133569861: 693,\n",
       "  0.6989754622805732: 694,\n",
       "  0.4710507585668971: 695,\n",
       "  0.8165435618926082: 696,\n",
       "  1.2843506293522167: 697,\n",
       "  0.7569563755863599: 698,\n",
       "  0.40239366444625574: 699,\n",
       "  0.5132187939865942: 700,\n",
       "  0.347942038527308: 701,\n",
       "  -0.7579189373999999: 702,\n",
       "  -0.8525242053000001: 703,\n",
       "  0.7241793856397005: 704,\n",
       "  0.8085070033059218: 705,\n",
       "  0.7046014456809201: 706,\n",
       "  0.42511172267746594: 707,\n",
       "  0.39596574196635953: 708,\n",
       "  0.5543546616253815: 709,\n",
       "  1.1417220200125229: 710,\n",
       "  0.5576431852438775: 711,\n",
       "  -0.5033975476: 712,\n",
       "  1.1875473674392492: 713,\n",
       "  0.6514677286125993: 714,\n",
       "  0.8059204031685212: 715,\n",
       "  0.5520267410016132: 716,\n",
       "  0.49654657075119357: 717,\n",
       "  0.7835737754928668: 718,\n",
       "  0.4389590340615334: 719,\n",
       "  1.2243515328559507: 720,\n",
       "  0.5071955382655867: 721,\n",
       "  0.42162469850000495: 722,\n",
       "  0.5828893060952506: 723,\n",
       "  0.4033776956153455: 724,\n",
       "  0.4443380168584844: 725,\n",
       "  0.4005911889338644: 726,\n",
       "  0.5992287140012823: 727,\n",
       "  1.0072160158487509: 728,\n",
       "  1.6670287436904323: 729,\n",
       "  0.6971400587165372: 730,\n",
       "  -0.8420275118: 731,\n",
       "  0.7534737734642154: 732,\n",
       "  0.9178954957551713: 733,\n",
       "  0.5546716510410381: 734,\n",
       "  -0.6061666574: 735,\n",
       "  1.5768698725760457: 736,\n",
       "  0.45346694487265476: 737,\n",
       "  0.4093870644882218: 738,\n",
       "  0.42107129211: 739,\n",
       "  -1.5166568280000001: 740,\n",
       "  1.5776350447973861: 741,\n",
       "  0.4816326830435904: 742,\n",
       "  0.3914720774104475: 743,\n",
       "  -0.8053411576: 744,\n",
       "  0.27315617313710433: 745,\n",
       "  0.624428137155205: 746,\n",
       "  0.3806019251176632: 747,\n",
       "  1.4267910304394782: 748,\n",
       "  0.4383444330121972: 749,\n",
       "  0.3676091927213208: 750,\n",
       "  0.7228496538031672: 751,\n",
       "  0.9821277921451628: 752,\n",
       "  0.4002517608670221: 753,\n",
       "  0.4652300206790055: 754,\n",
       "  -0.6927755889: 755,\n",
       "  0.8526586399524907: 756,\n",
       "  0.7956481501613719: 757,\n",
       "  0.969084486463706: 758,\n",
       "  -0.8384484884000001: 759,\n",
       "  -0.6818939352: 760,\n",
       "  -0.8968349995999999: 761,\n",
       "  0.3756885207930336: 762,\n",
       "  0.6327536404153316: 763,\n",
       "  0.30325813408503494: 764,\n",
       "  0.3961921021793774: 765,\n",
       "  0.23698307164199317: 766,\n",
       "  -0.6953356999: 767,\n",
       "  0.6444953670683091: 768,\n",
       "  -0.6146792485: 769,\n",
       "  0.8626688958678523: 770,\n",
       "  1.11769716487: 771,\n",
       "  0.3930838028351117: 772,\n",
       "  -0.9142128439: 773,\n",
       "  0.5455844069113366: 774,\n",
       "  -1.3195056049: 775,\n",
       "  -0.7646300353000001: 776,\n",
       "  0.8814314404591711: 777,\n",
       "  0.722728880763167: 778,\n",
       "  3.367815585530814: 779,\n",
       "  -0.8087844159: 780,\n",
       "  0.36941040801809377: 781,\n",
       "  0.4883761463476496: 782,\n",
       "  -0.966987099: 783,\n",
       "  0.28605965559403584: 784,\n",
       "  0.7031441526764004: 785,\n",
       "  0.7675440803876983: 786,\n",
       "  0.3246103831108052: 787,\n",
       "  0.37728491221404303: 788,\n",
       "  -0.3399683991: 789,\n",
       "  0.4724430964591093: 790,\n",
       "  2.184994358983448: 791,\n",
       "  0.5940469788214138: 792,\n",
       "  1.8942742128288035: 793,\n",
       "  -0.8115375418: 794,\n",
       "  0.37046751229827357: 795,\n",
       "  -0.6778673412: 796,\n",
       "  1.56155189414127: 797,\n",
       "  0.47060113826541455: 798,\n",
       "  1.131147511393273: 799,\n",
       "  0.5054143572054423: 800,\n",
       "  0.5427582030308234: 801,\n",
       "  0.5422655356418793: 802,\n",
       "  0.868867143812264: 803,\n",
       "  1.0529194703587827: 804,\n",
       "  -0.5429297627: 805,\n",
       "  0.6438877630690641: 806,\n",
       "  0.5781455351976935: 807,\n",
       "  -0.6656414293: 808,\n",
       "  0.552064448143451: 809,\n",
       "  1.0242903585589684: 810,\n",
       "  0.38081564808819496: 811,\n",
       "  0.42335234515455467: 812,\n",
       "  0.4276992552156175: 813,\n",
       "  0.7492134649441816: 814,\n",
       "  0.442903102951952: 815,\n",
       "  0.998542028535172: 816,\n",
       "  -0.794414905: 817,\n",
       "  0.46574166890753804: 818,\n",
       "  0.8234833448669275: 819,\n",
       "  0.2951333481176221: 820,\n",
       "  0.7368882285661889: 821,\n",
       "  0.7582860969734375: 822,\n",
       "  0.27519480324374473: 823,\n",
       "  1.0309409755057648: 824,\n",
       "  0.4215411021433948: 825,\n",
       "  0.5941966241016274: 826,\n",
       "  0.7448039705218745: 827,\n",
       "  0.5569018954088627: 828,\n",
       "  0.7892077606674333: 829,\n",
       "  0.8589583230650669: 830,\n",
       "  -1.5140526318: 831,\n",
       "  0.9690166854520177: 832,\n",
       "  -1.0946432303: 833,\n",
       "  0.3223228900331503: 834,\n",
       "  0.45135036503366693: 835,\n",
       "  0.35434872163487474: 836,\n",
       "  0.6207714080100737: 837,\n",
       "  1.2586712923002354: 838,\n",
       "  0.5093963749640212: 839,\n",
       "  0.3123118028095423: 840,\n",
       "  0.37335252783126893: 841,\n",
       "  -0.6751798045999999: 842,\n",
       "  1.3490654048012614: 843,\n",
       "  0.587759965379782: 844,\n",
       "  0.5661120482993393: 845,\n",
       "  0.47664756134290454: 846,\n",
       "  0.4495733744126123: 847,\n",
       "  0.7529462361777279: 848,\n",
       "  0.6368135585063024: 849,\n",
       "  0.37301800607169794: 850,\n",
       "  -0.7665528720999999: 851,\n",
       "  1.0072811883543478: 852,\n",
       "  0.9810596027916781: 853,\n",
       "  0.9215568840901881: 854,\n",
       "  0.4547250213235627: 855,\n",
       "  0.4326062784656248: 856,\n",
       "  1.1040491542489221: 857,\n",
       "  0.47311058542058904: 858,\n",
       "  1.222977141529051: 859,\n",
       "  0.7322178750832069: 860,\n",
       "  -0.7966225190999999: 861,\n",
       "  0.9543914106680128: 862,\n",
       "  1.4148795916184378: 863,\n",
       "  0.41776860144291084: 864,\n",
       "  0.46197889172424006: 865,\n",
       "  0.9849264246408407: 866,\n",
       "  0.4340099430686606: 867,\n",
       "  0.5607178379623871: 868,\n",
       "  0.4156274506191078: 869,\n",
       "  0.8352899327254464: 870,\n",
       "  0.6565813555711781: 871,\n",
       "  0.4115475480198523: 872,\n",
       "  0.7916607909135337: 873,\n",
       "  -0.7509222979000001: 874,\n",
       "  0.8798882054000057: 875,\n",
       "  0.6472132021226251: 876,\n",
       "  0.9226018863162341: 877,\n",
       "  -0.6883955475: 878,\n",
       "  0.35608142668487147: 879,\n",
       "  0.3426017637410725: 880,\n",
       "  -1.2717650583: 881,\n",
       "  1.4446937676829992: 882,\n",
       "  1.2773908672472971: 883,\n",
       "  0.32367741852952353: 884,\n",
       "  0.8265985404973595: 885,\n",
       "  0.5133601430009893: 886,\n",
       "  0.5189109702135823: 887,\n",
       "  -0.7777244833: 888,\n",
       "  0.43508038106236385: 889,\n",
       "  0.2546245565121355: 890,\n",
       "  0.5428991395355215: 891,\n",
       "  0.5528706079708963: 892,\n",
       "  1.4631190415141377: 893,\n",
       "  1.7422547314981143: 894,\n",
       "  0.42078081632404835: 895,\n",
       "  0.47005294122092667: 896,\n",
       "  1.111904270968006: 897,\n",
       "  0.6330917714617409: 898,\n",
       "  0.33543075486862406: 899,\n",
       "  -0.9582654785: 900,\n",
       "  0.4995586120531294: 901,\n",
       "  0.4853305191633053: 902,\n",
       "  2.1179541145007317: 903,\n",
       "  0.42538154359490044: 904,\n",
       "  0.3157317193226068: 905,\n",
       "  0.9006903200551458: 906,\n",
       "  -0.9577867048: 907,\n",
       "  0.4895922996472415: 908,\n",
       "  0.8047356285651517: 909,\n",
       "  0.3011658647828729: 910,\n",
       "  0.5142063409982822: 911,\n",
       "  0.6663247229244688: 912,\n",
       "  0.39858323476170815: 913,\n",
       "  -0.7367563487000001: 914,\n",
       "  0.37198451838531327: 915,\n",
       "  0.7147391776053995: 916,\n",
       "  0.4724876741450972: 917,\n",
       "  -0.7729411468999999: 918,\n",
       "  0.4312493262790134: 919,\n",
       "  0.7496301256608837: 920,\n",
       "  1.4182882571184094: 921,\n",
       "  0.3706169471130793: 922,\n",
       "  0.40880040189251005: 923,\n",
       "  -0.7706656990999999: 924,\n",
       "  0.650900183208059: 925,\n",
       "  0.9223474728418151: 926,\n",
       "  -1.4343051994: 927,\n",
       "  0.8372239584736699: 928,\n",
       "  0.3395016401070058: 929,\n",
       "  0.5055004781860227: 930,\n",
       "  0.8561695006991691: 931,\n",
       "  -0.9229287733: 932,\n",
       "  0.5696701433434: 933,\n",
       "  0.25425358673103793: 934,\n",
       "  1.567411804569182: 935,\n",
       "  0.51861684295694: 936,\n",
       "  -0.9465902270000001: 937,\n",
       "  0.891670708734213: 938,\n",
       "  0.4844789578208779: 939,\n",
       "  0.36121968619347844: 940,\n",
       "  -0.7540935120000001: 941,\n",
       "  0.18619293039028562: 942,\n",
       "  0.8924001899542253: 943,\n",
       "  1.0751517947005527: 944,\n",
       "  0.26684537959051585: 945,\n",
       "  0.48372783014927356: 946,\n",
       "  0.7101047530548411: 947,\n",
       "  0.5760231083722862: 948,\n",
       "  0.47570094273970964: 949,\n",
       "  -0.7102845431: 950,\n",
       "  -0.8739811069: 951,\n",
       "  -0.6311319033: 952,\n",
       "  0.4053786108793939: 953,\n",
       "  0.9450337787590105: 954,\n",
       "  0.6498825684056682: 955,\n",
       "  0.706408387455742: 956,\n",
       "  -1.3908645269: 957,\n",
       "  0.6370828727652595: 958,\n",
       "  0.2658277361827518: 959,\n",
       "  0.4915642724093642: 960,\n",
       "  0.366194922980457: 961,\n",
       "  -1.0325387952: 962,\n",
       "  -1.4872043772: 963,\n",
       "  -0.6748278017: 964,\n",
       "  0.41792772666066136: 965,\n",
       "  -1.5818573663: 966,\n",
       "  0.6317482384981407: 967,\n",
       "  1.7465834416907677: 968,\n",
       "  0.6024377968524632: 969,\n",
       "  0.5873971138464373: 970,\n",
       "  1.0291582843148135: 971,\n",
       "  0.569669767565: 972,\n",
       "  0.2943638960806493: 973,\n",
       "  0.2960738046884313: 974,\n",
       "  -0.9780229678: 975,\n",
       "  1.2885374039310935: 976,\n",
       "  0.7616139174626954: 977,\n",
       "  0.5016824740179597: 978,\n",
       "  0.6083183915758118: 979,\n",
       "  0.5059798406910092: 980,\n",
       "  0.6804803540927216: 981,\n",
       "  0.35155131406922996: 982,\n",
       "  0.4719291503324526: 983,\n",
       "  -1.3630301683000001: 984,\n",
       "  -0.7181242784999999: 985,\n",
       "  0.3247998827965974: 986,\n",
       "  -0.8264356404000001: 987,\n",
       "  1.1449395149222372: 988,\n",
       "  0.7538430722325118: 989,\n",
       "  0.35735230721188166: 990,\n",
       "  0.35896493402250107: 991,\n",
       "  0.5045388256804789: 992,\n",
       "  -1.3116294831: 993,\n",
       "  0.4249917354406654: 994,\n",
       "  0.4325300949972118: 995,\n",
       "  0.378942191999767: 996,\n",
       "  -0.5304761621999999: 997,\n",
       "  0.8868715764293875: 998,\n",
       "  0.32489767142603476: 999,\n",
       "  -0.7666606779: 1000,\n",
       "  -0.7416894942: 1001,\n",
       "  0.4194030617654126: 1002,\n",
       "  0.470543634530847: 1003,\n",
       "  0.5892097943862787: 1004,\n",
       "  0.8220348275741054: 1005,\n",
       "  0.39069548539222915: 1006,\n",
       "  0.7739083220111358: 1007,\n",
       "  0.5567230902128588: 1008,\n",
       "  1.0648873325754784: 1009,\n",
       "  -0.8219358323: 1010,\n",
       "  1.3148637852844232: 1011,\n",
       "  0.45297068115784406: 1012,\n",
       "  1.0091417754201837: 1013,\n",
       "  0.40349510927802273: 1014,\n",
       "  0.3892321987787216: 1015,\n",
       "  1.63541072322: 1016,\n",
       "  -2.0552643553: 1017,\n",
       "  0.5595969508774099: 1018,\n",
       "  0.31915318449435043: 1019,\n",
       "  0.5276487076008078: 1020,\n",
       "  0.4443969683025: 1021,\n",
       "  0.5758680453753501: 1022,\n",
       "  0.9331974958867653: 1023,\n",
       "  1.2931206947339924: 1024,\n",
       "  -0.4285763479: 1025,\n",
       "  0.5344534553129999: 1026,\n",
       "  -0.6719590208: 1027,\n",
       "  0.4848202199566957: 1028,\n",
       "  -0.8424065290999999: 1029,\n",
       "  0.47843494854186847: 1030,\n",
       "  0.8231343951450408: 1031,\n",
       "  1.11185084122211: 1032,\n",
       "  0.37654502656070415: 1033,\n",
       "  0.44921784429496664: 1034,\n",
       "  1.0158486610655573: 1035,\n",
       "  0.7063987720621588: 1036,\n",
       "  -1.2635358037: 1037,\n",
       "  0.3359021519717773: 1038,\n",
       "  0.4586792568377817: 1039,\n",
       "  0.7558980723355957: 1040,\n",
       "  1.7074833470751947: 1041,\n",
       "  0.5179195283973457: 1042,\n",
       "  0.7759201416239518: 1043,\n",
       "  -0.4268857048: 1044,\n",
       "  -0.5872614691: 1045,\n",
       "  -0.9146760254: 1046,\n",
       "  1.1661216690280254: 1047,\n",
       "  1.211545376967863: 1048,\n",
       "  0.491521070041936: 1049,\n",
       "  -0.8882793981999999: 1050,\n",
       "  -1.1963106164: 1051,\n",
       "  0.7562729659392761: 1052,\n",
       "  0.8793774064651005: 1053,\n",
       "  0.5828886636861057: 1054,\n",
       "  0.5951621093918491: 1055,\n",
       "  0.33958987170680904: 1056,\n",
       "  -0.9511411154: 1057,\n",
       "  0.5348447774243911: 1058,\n",
       "  0.4442029320208518: 1059,\n",
       "  0.4469714143751925: 1060,\n",
       "  0.8151033071418494: 1061,\n",
       "  1.154960890735634: 1062,\n",
       "  0.5781846769617093: 1063,\n",
       "  0.8457052567428539: 1064,\n",
       "  0.3662975246677799: 1065,\n",
       "  1.2654590048223135: 1066,\n",
       "  0.47561926793090376: 1067,\n",
       "  0.7243131721307959: 1068,\n",
       "  0.4602823199357287: 1069,\n",
       "  0.5380264815140465: 1070,\n",
       "  1.0929010577185845: 1071,\n",
       "  0.5276981043231579: 1072,\n",
       "  0.3512702745740845: 1073,\n",
       "  0.6612840861723983: 1074,\n",
       "  0.7780570206413353: 1075,\n",
       "  0.457983433007856: 1076,\n",
       "  0.25816741316334413: 1077,\n",
       "  0.6475470543163314: 1078,\n",
       "  -0.6191669957: 1079,\n",
       "  -0.7930222554: 1080,\n",
       "  0.6191386832778307: 1081,\n",
       "  1.2428414025663792: 1082,\n",
       "  0.4513148404792928: 1083,\n",
       "  0.19706688304383818: 1084,\n",
       "  1.1566288021517566: 1085,\n",
       "  0.39374299809512103: 1086,\n",
       "  0.5447226623580131: 1087,\n",
       "  0.9138624776794114: 1088,\n",
       "  0.452665202047443: 1089,\n",
       "  -0.6011009184: 1090,\n",
       "  -0.7558839443: 1091,\n",
       "  0.22478008080518438: 1092,\n",
       "  1.2154801281808596: 1093,\n",
       "  0.5634111078239338: 1094,\n",
       "  1.2140236443499022: 1095,\n",
       "  -0.6800127612000001: 1096,\n",
       "  0.2944446902301048: 1097,\n",
       "  2.4319775861399378: 1098,\n",
       "  1.9114721837088111: 1099,\n",
       "  -0.7225271307: 1100,\n",
       "  0.5008837746872613: 1101,\n",
       "  0.5721693485741243: 1102,\n",
       "  0.9507375277219523: 1103,\n",
       "  1.130775318789841: 1104,\n",
       "  0.6014508328871992: 1105,\n",
       "  0.8024037042285899: 1106,\n",
       "  0.3479138314075392: 1107,\n",
       "  0.6374471295778702: 1108,\n",
       "  -0.5629761529: 1109,\n",
       "  -0.7396531462999999: 1110,\n",
       "  0.37406151503144175: 1111,\n",
       "  0.31824297299608034: 1112,\n",
       "  -1.4819854897: 1113,\n",
       "  0.7881474611248143: 1114,\n",
       "  0.7375533318904098: 1115,\n",
       "  0.5102460262322364: 1116,\n",
       "  0.7006745938226053: 1117,\n",
       "  4.390462521893984: 1118,\n",
       "  0.5737582036774234: 1119,\n",
       "  0.5110882086471292: 1120,\n",
       "  0.41134919618647375: 1121,\n",
       "  -0.8860792084: 1122,\n",
       "  0.4049221944019891: 1123,\n",
       "  0.3965436934373736: 1124,\n",
       "  0.5795164319584682: 1125,\n",
       "  0.23018736193040917: 1126,\n",
       "  -1.1701786965: 1127,\n",
       "  1.270253358600867: 1128,\n",
       "  -0.9426751103000001: 1129,\n",
       "  -0.8082158189: 1130,\n",
       "  0.4984156717523561: 1131,\n",
       "  -0.5675997018: 1132,\n",
       "  -0.8158806022: 1133,\n",
       "  1.7490024938391702: 1134,\n",
       "  0.4062008711757688: 1135,\n",
       "  0.45311693136448655: 1136,\n",
       "  0.6536778103836436: 1137,\n",
       "  0.5666990994162943: 1138,\n",
       "  1.0739119036970712: 1139,\n",
       "  0.32123284357111165: 1140,\n",
       "  0.8072166749739288: 1141,\n",
       "  0.8630021594788392: 1142,\n",
       "  0.6788478290468382: 1143,\n",
       "  0.513760795415006: 1144,\n",
       "  0.8134144171524371: 1145,\n",
       "  0.42167794982295864: 1146,\n",
       "  -0.6447819799: 1147,\n",
       "  0.8290830124594382: 1148,\n",
       "  0.5474217131470384: 1149,\n",
       "  -0.4912877328: 1150,\n",
       "  0.2803087555572646: 1151,\n",
       "  -0.7040467443: 1152,\n",
       "  0.5036374949838922: 1153,\n",
       "  -0.40190661310000003: 1154,\n",
       "  0.8328368110970574: 1155,\n",
       "  0.2745268516019388: 1156,\n",
       "  0.6406786174737357: 1157,\n",
       "  0.8349680179686073: 1158,\n",
       "  -0.7258651151000001: 1159,\n",
       "  -0.8907467385: 1160,\n",
       "  2.539247398440117: 1161,\n",
       "  0.9594975630974952: 1162,\n",
       "  0.42960109667007: 1163,\n",
       "  1.5040542446303726: 1164,\n",
       "  0.39844571472815893: 1165,\n",
       "  0.6731890307258058: 1166,\n",
       "  0.42124860524834157: 1167,\n",
       "  0.36100870580326877: 1168,\n",
       "  0.6236221588855183: 1169,\n",
       "  -0.8900622134: 1170,\n",
       "  ...},\n",
       " 'ps_ind_01': {2: 11980,\n",
       "  1: 11981,\n",
       "  5: 11982,\n",
       "  0: 11983,\n",
       "  4: 11984,\n",
       "  3: 11985,\n",
       "  6: 11986,\n",
       "  7: 11987},\n",
       " 'ps_ind_02_cat': {2: 11988, 1: 11989, 4: 11990, 3: 11991, -1: 11992},\n",
       " 'ps_ind_03': {5: 11993,\n",
       "  7: 11994,\n",
       "  9: 11995,\n",
       "  2: 11996,\n",
       "  0: 11997,\n",
       "  4: 11998,\n",
       "  3: 11999,\n",
       "  1: 12000,\n",
       "  11: 12001,\n",
       "  6: 12002,\n",
       "  8: 12003,\n",
       "  10: 12004},\n",
       " 'ps_ind_04_cat': {1: 12005, 0: 12006, -1: 12007},\n",
       " 'ps_ind_05_cat': {0: 12008,\n",
       "  1: 12009,\n",
       "  4: 12010,\n",
       "  3: 12011,\n",
       "  6: 12012,\n",
       "  5: 12013,\n",
       "  -1: 12014,\n",
       "  2: 12015},\n",
       " 'ps_ind_06_bin': {0: 12016, 1: 12017},\n",
       " 'ps_ind_07_bin': {1: 12018, 0: 12019},\n",
       " 'ps_ind_08_bin': {0: 12020, 1: 12021},\n",
       " 'ps_ind_09_bin': {0: 12022, 1: 12023},\n",
       " 'ps_ind_10_bin': {0: 12024, 1: 12025},\n",
       " 'ps_ind_11_bin': {0: 12026, 1: 12027},\n",
       " 'ps_ind_12_bin': {0: 12028, 1: 12029},\n",
       " 'ps_ind_13_bin': {0: 12030, 1: 12031},\n",
       " 'ps_ind_14': {0: 12032, 1: 12033, 2: 12034, 3: 12035},\n",
       " 'ps_ind_15': {11: 12036,\n",
       "  3: 12037,\n",
       "  12: 12038,\n",
       "  8: 12039,\n",
       "  9: 12040,\n",
       "  6: 12041,\n",
       "  13: 12042,\n",
       "  4: 12043,\n",
       "  10: 12044,\n",
       "  5: 12045,\n",
       "  7: 12046,\n",
       "  2: 12047,\n",
       "  0: 12048,\n",
       "  1: 12049},\n",
       " 'ps_ind_16_bin': {0: 12050, 1: 12051},\n",
       " 'ps_ind_17_bin': {1: 12052, 0: 12053},\n",
       " 'ps_ind_18_bin': {0: 12054, 1: 12055}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_Xi_train, cate_Xv_train, numeric_Xv_train, y_train = data_parser.parse(df=dfTrain, has_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_Xi_test, cate_Xv_test, numeric_Xv_test= data_parser.parse(df=dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <th>ps_car_04_cat</th>\n",
       "      <th>ps_car_05_cat</th>\n",
       "      <th>ps_car_06_cat</th>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <th>ps_car_08_cat</th>\n",
       "      <th>ps_car_09_cat</th>\n",
       "      <th>ps_car_10_cat</th>\n",
       "      <th>ps_car_11_cat</th>\n",
       "      <th>ps_car_11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n",
       "0          2              2          5              1              0   \n",
       "1          1              1          7              0              0   \n",
       "2          5              4          9              1              0   \n",
       "3          0              1          2              0              0   \n",
       "4          0              2          0              1              0   \n",
       "\n",
       "   ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  ps_ind_10_bin  \\\n",
       "0              0              1              0              0              0   \n",
       "1              0              0              1              0              0   \n",
       "2              0              0              1              0              0   \n",
       "3              1              0              0              0              0   \n",
       "4              1              0              0              0              0   \n",
       "\n",
       "     ...      ps_car_03_cat  ps_car_04_cat  ps_car_05_cat  ps_car_06_cat  \\\n",
       "0    ...                 -1              0              1              4   \n",
       "1    ...                 -1              0             -1             11   \n",
       "2    ...                 -1              0             -1             14   \n",
       "3    ...                  0              0              1             11   \n",
       "4    ...                 -1              0             -1             14   \n",
       "\n",
       "   ps_car_07_cat  ps_car_08_cat  ps_car_09_cat  ps_car_10_cat  ps_car_11_cat  \\\n",
       "0              1              0              0              1             12   \n",
       "1              1              1              2              1             19   \n",
       "2              1              1              2              1             60   \n",
       "3              1              1              3              1            104   \n",
       "4              1              1              2              1             82   \n",
       "\n",
       "   ps_car_11  \n",
       "0          2  \n",
       "1          3  \n",
       "2          1  \n",
       "3          1  \n",
       "4          3  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain[[col for col in dfTrain.columns if (col not in IGNORE_COLS) and (col not in NUMERIC_COLS)]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cate_Xi_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12056"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.feat_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcn_params = {\n",
    "    \"embedding_size\": 8,\n",
    "    \"deep_layers\": [32, 32],\n",
    "    \"dropout_deep\": [0.5, 0.5, 0.5],\n",
    "    \"deep_layers_activation\": tf.nn.relu,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer_type\": \"adam\",\n",
    "    \"batch_norm\": 1,\n",
    "    \"batch_norm_decay\": 0.995,\n",
    "    \"l2_reg\": 0.01,\n",
    "    \"verbose\": True,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"cross_layer_num\":3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcn_params['cate_feature_size'] = fd.feat_dim # category feature的中不同类别的总个数，相当于词字典的词数量。\n",
    "dcn_params['field_size'] = len(cate_Xi_train[0]) # field为有多少个category feature 每一列feature当做一个field。相当于有30个field的12056词。\n",
    "dcn_params['numeric_feature_size'] = len(NUMERIC_COLS) # numeric feature的个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取验证集与训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义匿名函数，目的获取已知索引的对应特征，获取训练集与验证集\n",
    "_get = lambda x, l:[x[i] for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = list(StratifiedKFold(n_splits=NUM_SPLITS, shuffle=True,random_state=RANDOM_SEED).split(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#params:119336\n",
      "6666\n",
      "6666\n",
      "6666\n",
      "6666\n",
      "epoch 0 loss [0.7339406]\n",
      "epoch 1 loss [0.689344]\n",
      "epoch 2 loss [0.6579944]\n",
      "epoch 3 loss [0.6139721]\n",
      "epoch 4 loss [0.57330567]\n",
      "epoch 5 loss [0.5425892]\n",
      "epoch 6 loss [0.51661056]\n",
      "epoch 7 loss [0.49185112]\n",
      "epoch 8 loss [0.46937397]\n",
      "epoch 9 loss [0.44965577]\n",
      "epoch 10 loss [0.43133047]\n",
      "epoch 11 loss [0.41413397]\n",
      "epoch 12 loss [0.39858332]\n",
      "epoch 13 loss [0.38438743]\n",
      "epoch 14 loss [0.37130785]\n",
      "epoch 15 loss [0.3591925]\n",
      "epoch 16 loss [0.3479624]\n",
      "epoch 17 loss [0.33759972]\n",
      "epoch 18 loss [0.32794872]\n",
      "epoch 19 loss [0.31895694]\n",
      "epoch 20 loss [0.31060877]\n",
      "epoch 21 loss [0.30281362]\n",
      "epoch 22 loss [0.2955329]\n",
      "epoch 23 loss [0.28878072]\n",
      "epoch 24 loss [0.28245845]\n",
      "epoch 25 loss [0.2765365]\n",
      "epoch 26 loss [0.27101922]\n",
      "epoch 27 loss [0.26581767]\n",
      "epoch 28 loss [0.2610159]\n",
      "epoch 29 loss [0.2565304]\n",
      "#params:119336\n",
      "6667\n",
      "6667\n",
      "6667\n",
      "6667\n",
      "epoch 0 loss [1.0582225]\n",
      "epoch 1 loss [0.7997861]\n",
      "epoch 2 loss [0.66108996]\n",
      "epoch 3 loss [0.5902871]\n",
      "epoch 4 loss [0.55408746]\n",
      "epoch 5 loss [0.5289075]\n",
      "epoch 6 loss [0.5046325]\n",
      "epoch 7 loss [0.48079178]\n",
      "epoch 8 loss [0.4590978]\n",
      "epoch 9 loss [0.4399867]\n",
      "epoch 10 loss [0.4229444]\n",
      "epoch 11 loss [0.4073837]\n",
      "epoch 12 loss [0.39274475]\n",
      "epoch 13 loss [0.37912783]\n",
      "epoch 14 loss [0.36660668]\n",
      "epoch 15 loss [0.35507107]\n",
      "epoch 16 loss [0.344392]\n",
      "epoch 17 loss [0.334459]\n",
      "epoch 18 loss [0.3252165]\n",
      "epoch 19 loss [0.31659463]\n",
      "epoch 20 loss [0.3085465]\n",
      "epoch 21 loss [0.30103463]\n",
      "epoch 22 loss [0.2939967]\n",
      "epoch 23 loss [0.2873774]\n",
      "epoch 24 loss [0.2811773]\n",
      "epoch 25 loss [0.2753652]\n",
      "epoch 26 loss [0.2699001]\n",
      "epoch 27 loss [0.2647573]\n",
      "epoch 28 loss [0.25993314]\n",
      "epoch 29 loss [0.25535712]\n",
      "#params:119336\n",
      "6667\n",
      "6667\n",
      "6667\n",
      "6667\n",
      "epoch 0 loss [0.76318055]\n",
      "epoch 1 loss [0.66678566]\n",
      "epoch 2 loss [0.61786705]\n",
      "epoch 3 loss [0.5862076]\n",
      "epoch 4 loss [0.5563791]\n",
      "epoch 5 loss [0.52657825]\n",
      "epoch 6 loss [0.4997391]\n",
      "epoch 7 loss [0.47598255]\n",
      "epoch 8 loss [0.45465443]\n",
      "epoch 9 loss [0.4351559]\n",
      "epoch 10 loss [0.4171612]\n",
      "epoch 11 loss [0.40066358]\n",
      "epoch 12 loss [0.38555008]\n",
      "epoch 13 loss [0.37157634]\n",
      "epoch 14 loss [0.3587175]\n",
      "epoch 15 loss [0.34667665]\n",
      "epoch 16 loss [0.33555996]\n",
      "epoch 17 loss [0.32522327]\n",
      "epoch 18 loss [0.3157626]\n",
      "epoch 19 loss [0.30704737]\n",
      "epoch 20 loss [0.29887164]\n",
      "epoch 21 loss [0.29118055]\n",
      "epoch 22 loss [0.28421202]\n",
      "epoch 23 loss [0.27754846]\n",
      "epoch 24 loss [0.2715385]\n",
      "epoch 25 loss [0.26589105]\n",
      "epoch 26 loss [0.26066273]\n",
      "epoch 27 loss [0.25579464]\n",
      "epoch 28 loss [0.25106698]\n",
      "epoch 29 loss [0.24702118]\n"
     ]
    }
   ],
   "source": [
    "for i, (train_idx, valid_idx) in enumerate(folds):\n",
    "    cate_Xi_train_, cate_Xv_train_, numeric_Xv_train_, y_train_ = _get(cate_Xi_train,train_idx), _get(cate_Xv_train,train_idx),_get(numeric_Xv_train,train_idx), _get(y_train,train_idx)\n",
    "    cate_Xi_valid_, cate_Xv_valid_, numeric_Xv_valid_, y_valid_ = _get(cate_Xi_train,valid_idx), _get(cate_Xv_train,valid_idx),_get(numeric_Xv_train,valid_idx), _get(y_train,valid_idx)\n",
    "    \n",
    "    dcn =  DCN(**dcn_params)\n",
    "    dcn.fit(cate_Xi_train_, cate_Xv_train_, numeric_Xv_train_,y_train_, cate_Xi_valid_, cate_Xv_valid_, numeric_Xv_valid_,y_valid_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
