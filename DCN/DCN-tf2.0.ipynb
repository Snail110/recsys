{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\julianxu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,output_dim,num_layer,**kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layer = num_layer\n",
    "        super(CrossLayer,self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        self.input_dim = input_shape[1]\n",
    "        # print(self.input_dim)\n",
    "        self.W = []\n",
    "        self.bias = []\n",
    "        for i in range(self.num_layer):\n",
    "            self.W.append(self.add_weight(shape=[self.input_dim,1],initializer = 'glorot_uniform',name='w_{}'.format(i),trainable=True))\n",
    "            self.bias.append(self.add_weight(shape=[self.input_dim,1],initializer = 'zeros',name='b_{}'.format(i),trainable=True))\n",
    "        self.built = True\n",
    "    def call(self,input):\n",
    "\n",
    "        x0 = tf.einsum('bij->bji',input) # output[j][i] = m[i][j]\n",
    "        # print(\"x0_shape\",x0.get_shape())\n",
    "        x1 = tf.einsum('bmn,bnk->bmk',input,x0)\n",
    "        cross = tf.einsum('bmn,nk->bmk',x1,self.W[0]) + self.bias[0] + input\n",
    "        \n",
    "        for i in range(1,self.num_layer):\n",
    "            x0 = tf.einsum('bij->bji',cross) # output[j][i] = m[i][j]\n",
    "            x1 = tf.einsum('bmn,bnk->bmk',input,x0)\n",
    "            cross = tf.einsum('bmn,nk->bmk',x1,self.W[i]) + self.bias[i] + cross\n",
    "        return cross\n",
    "        \n",
    "class Deep(tf.keras.layers.Layer):\n",
    "    def __init__(self,dropout_deep,deep_layer_sizes):\n",
    "        # input_dim = num_size + embed_size = input_size\n",
    "        super(Deep, self).__init__()\n",
    "        self.dropout_deep  = dropout_deep\n",
    "        # fc layer\n",
    "        self.deep_layer_sizes = deep_layer_sizes\n",
    "        # 神经网络方面的参数\n",
    "        for i in range(len(deep_layer_sizes)):\n",
    "            setattr(self, 'dense_' + str(i),tf.keras.layers.Dense(deep_layer_sizes[i]))\n",
    "            setattr(self, 'batchNorm_' + str(i),tf.keras.layers.BatchNormalization())\n",
    "            setattr(self, 'activation_' + str(i),tf.keras.layers.Activation('relu'))\n",
    "            setattr(self, 'dropout_' + str(i),tf.keras.layers.Dropout(dropout_deep[i]))\n",
    "        # last layer\n",
    "        self.fc = tf.keras.layers.Dense(128,activation=None,use_bias=True)\n",
    "        \n",
    "    def call(self,input):\n",
    "        y_deep = getattr(self,'dense_' + str(0))(input)\n",
    "        y_deep = getattr(self,'batchNorm_' + str(0))(y_deep)\n",
    "        y_deep = getattr(self,'activation_' + str(0))(y_deep)\n",
    "        y_deep = getattr(self,'dropout_' + str(0))(y_deep)\n",
    "        \n",
    "        for i in range(1,len(self.deep_layer_sizes)):\n",
    "            y_deep = getattr(self,'dense_' + str(i))(y_deep)\n",
    "            y_deep = getattr(self,'batchNorm_' + str(i))(y_deep)\n",
    "            y_deep = getattr(self,'activation_' + str(i))(y_deep)\n",
    "            y_deep = getattr(self,'dropout_' + str(i))(y_deep)\n",
    "        \n",
    "        output = self.fc(y_deep)\n",
    "        return output\n",
    "    \n",
    "class DCN(tf.keras.Model):\n",
    "    def __init__(self,num_feat,num_field,dropout_deep,deep_layer_sizes,embedding_size=10):\n",
    "        super().__init__()\n",
    "        self.num_feat = num_feat # F =features nums\n",
    "        self.num_field = num_field # N =fields of a feature \n",
    "        self.dropout_deep  = dropout_deep\n",
    "        \n",
    "        # Embedding 这里采用embeddings层因此大小为F* M F为特征数量，M为embedding的维度\n",
    "        feat_embeddings = tf.keras.layers.Embedding(num_feat, embedding_size, embeddings_initializer='uniform') # F * M \n",
    "        self.feat_embeddings = feat_embeddings\n",
    "        \n",
    "        self.crosslayer = CrossLayer(output_dim = 128,num_layer=8)\n",
    "    \n",
    "        self.deep = Deep(dropout_deep,deep_layer_sizes)\n",
    "        self.fc = tf.keras.layers.Dense(1,activation='sigmoid',use_bias=True)\n",
    "        \n",
    "    def call(self,feat_index,feat_value):\n",
    "        \n",
    "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
    "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
    "#         print(feat_value.get_shape())\n",
    "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
    "        # print(\"feat_embedding:\",feat_embedding.get_shape()) # 32 * 39 * 10\n",
    "        stack_input = tf.keras.layers.Reshape((1,-1))(feat_embedding)\n",
    "        # print(\"stack_input:\",stack_input.get_shape()) # 32 * 1 * 390\n",
    "        \n",
    "        x1 = self.crosslayer(stack_input)\n",
    "        x2 = self.deep(stack_input)\n",
    "        \n",
    "        x3 = tf.keras.layers.concatenate([x1,x2],axis=-1)\n",
    "        output = self.fc(x3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_table('../data/Criteo/train.txt')\n",
    "train.columns=['label','I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9',\n",
    "       'I10', 'I11', 'I12', 'I13','C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7',\n",
    "       'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17',\n",
    "       'C18', 'C19', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features=['I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9',\n",
    "       'I10', 'I11', 'I12', 'I13']\n",
    "dist_features = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7',\n",
    "       'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17',\n",
    "       'C18', 'C19', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_ = 10\n",
    "# dir_feat_dict_ = 'feat_dict_' + str(freq_) + '.pkl2'\n",
    "continuous_range_ = range(1, 14)\n",
    "categorical_range_ = range(14, 40)\n",
    "\n",
    "# 统计离散特征每个离散值出现的次数组成字典\n",
    "feat_cnt = Counter()\n",
    "with open('../data/Criteo/train.txt', 'r') as fin:\n",
    "    for line_idx, line in enumerate(fin):\n",
    "        features = line.rstrip('\\n').split('\\t')\n",
    "        for idx in categorical_range_:\n",
    "            if features[idx] == '': continue\n",
    "            feat_cnt.update([features[idx]])\n",
    "# Only retain discrete features with high frequency\n",
    "dis_feat_set = set() # 高频段的离散字符\n",
    "for feat, ot in feat_cnt.items():\n",
    "    if ot >= freq_:\n",
    "        dis_feat_set.add(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for continuous and discrete features\n",
    "feat_dict = {}\n",
    "tc = 1\n",
    "# Continuous features\n",
    "for idx in continuous_range_:\n",
    "    feat_dict[idx] = tc\n",
    "    tc += 1 # 代表占据一列\n",
    "\n",
    "# Discrete features\n",
    "cnt_feat_set = set()\n",
    "with open('../data/Criteo/train.txt', 'r') as fin:\n",
    "    for line_idx, line in enumerate(fin):\n",
    "        features = line.rstrip('\\n').split('\\t')\n",
    "        for idx in categorical_range_:\n",
    "            # 排除空字符和低频离散字符\n",
    "            if features[idx] == '' or features[idx] not in dis_feat_set:\n",
    "                continue\n",
    "            # 排除连续性数值\n",
    "            if features[idx] not in cnt_feat_set:\n",
    "                cnt_feat_set.add(features[idx])\n",
    "                # 获取种类数\n",
    "                feat_dict[features[idx]] = tc\n",
    "                tc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = []\n",
    "train_value = []\n",
    "train_idx = []\n",
    "\n",
    "continuous_range_ = range(1, 14)\n",
    "categorical_range_ = range(14, 40)\n",
    "cont_max_=[]\n",
    "cont_min_=[]\n",
    "for cf in cont_features:\n",
    "    cont_max_.append(max(train[cf]))\n",
    "    cont_min_.append(min(train[cf]))\n",
    "cont_diff_ = [cont_max_[i] - cont_min_[i] for i in range(len(cont_min_))]\n",
    "\n",
    "def process_line_(line):\n",
    "    features = line.rstrip('\\n').split('\\t')\n",
    "    feat_idx, feat_value, label = [], [], []\n",
    "\n",
    "    # MinMax Normalization\n",
    "    for idx in continuous_range_:\n",
    "        if features[idx] == '':\n",
    "            feat_idx.append(0)\n",
    "            feat_value.append(0.0)\n",
    "        else:\n",
    "            feat_idx.append(feat_dict[idx])\n",
    "            # 归一化\n",
    "            feat_value.append(round((float(features[idx]) - cont_min_[idx - 1]) / cont_diff_[idx - 1], 6))\n",
    "\n",
    "    # 处理离散型数据\n",
    "    for idx in categorical_range_:\n",
    "        if features[idx] == '' or features[idx] not in feat_dict:\n",
    "            feat_idx.append(0)\n",
    "            feat_value.append(0.0)\n",
    "        else:\n",
    "            feat_idx.append(feat_dict[features[idx]])\n",
    "            feat_value.append(1.0)\n",
    "    return feat_idx, feat_value, [int(features[0])]\n",
    "\n",
    "with open('../data/Criteo/train.txt', 'r') as fin:\n",
    "    for line_idx, line in enumerate(fin):\n",
    "\n",
    "        feat_idx, feat_value, label = process_line_(line)\n",
    "        train_label.append(label)\n",
    "        train_idx.append(feat_idx)\n",
    "        train_value.append(feat_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcn= DCN(num_feat=len(feat_dict) + 1, num_field=39, dropout_deep=[0.5, 0.5, 0.5],\n",
    "                deep_layer_sizes=[400, 400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_label,train_idx,train_value)).shuffle(10000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_one_step(model, optimizer, idx, value, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(idx,value)\n",
    "        loss = loss_object(y_true=label, y_pred=output)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    grads = [tf.clip_by_norm(g, 100) for g in grads]\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(label,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_acc')\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method DCN.call of <__main__.DCN object at 0x00000183593F28D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method DCN.call of <__main__.DCN object at 0x00000183593F28D0>>, which Python reported as:\n",
      "    def call(self,feat_index,feat_value):\n",
      "        \n",
      "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
      "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
      "#         print(feat_value.get_shape())\n",
      "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
      "        # print(\"feat_embedding:\",feat_embedding.get_shape()) # 32 * 39 * 10\n",
      "        stack_input = tf.keras.layers.Reshape((1,-1))(feat_embedding)\n",
      "        # print(\"stack_input:\",stack_input.get_shape()) # 32 * 1 * 390\n",
      "        \n",
      "        x1 = self.crosslayer(stack_input)\n",
      "        x2 = self.deep(stack_input)\n",
      "        \n",
      "        x3 = tf.keras.layers.concatenate([x1,x2],axis=-1)\n",
      "        output = self.fc(x3)\n",
      "        return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method DCN.call of <__main__.DCN object at 0x00000183593F28D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method DCN.call of <__main__.DCN object at 0x00000183593F28D0>>, which Python reported as:\n",
      "    def call(self,feat_index,feat_value):\n",
      "        \n",
      "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
      "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
      "#         print(feat_value.get_shape())\n",
      "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
      "        # print(\"feat_embedding:\",feat_embedding.get_shape()) # 32 * 39 * 10\n",
      "        stack_input = tf.keras.layers.Reshape((1,-1))(feat_embedding)\n",
      "        # print(\"stack_input:\",stack_input.get_shape()) # 32 * 1 * 390\n",
      "        \n",
      "        x1 = self.crosslayer(stack_input)\n",
      "        x2 = self.deep(stack_input)\n",
      "        \n",
      "        x3 = tf.keras.layers.concatenate([x1,x2],axis=-1)\n",
      "        output = self.fc(x3)\n",
      "        return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method DCN.call of <__main__.DCN object at 0x00000183593F28D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method DCN.call of <__main__.DCN object at 0x00000183593F28D0>>, which Python reported as:\n",
      "    def call(self,feat_index,feat_value):\n",
      "        \n",
      "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
      "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
      "#         print(feat_value.get_shape())\n",
      "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
      "        # print(\"feat_embedding:\",feat_embedding.get_shape()) # 32 * 39 * 10\n",
      "        stack_input = tf.keras.layers.Reshape((1,-1))(feat_embedding)\n",
      "        # print(\"stack_input:\",stack_input.get_shape()) # 32 * 1 * 390\n",
      "        \n",
      "        x1 = self.crosslayer(stack_input)\n",
      "        x2 = self.deep(stack_input)\n",
      "        \n",
      "        x3 = tf.keras.layers.concatenate([x1,x2],axis=-1)\n",
      "        output = self.fc(x3)\n",
      "        return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method DCN.call of <__main__.DCN object at 0x00000183593F28D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method DCN.call of <__main__.DCN object at 0x00000183593F28D0>>, which Python reported as:\n",
      "    def call(self,feat_index,feat_value):\n",
      "        \n",
      "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
      "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
      "#         print(feat_value.get_shape())\n",
      "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
      "        # print(\"feat_embedding:\",feat_embedding.get_shape()) # 32 * 39 * 10\n",
      "        stack_input = tf.keras.layers.Reshape((1,-1))(feat_embedding)\n",
      "        # print(\"stack_input:\",stack_input.get_shape()) # 32 * 1 * 390\n",
      "        \n",
      "        x1 = self.crosslayer(stack_input)\n",
      "        x2 = self.deep(stack_input)\n",
      "        \n",
      "        x3 = tf.keras.layers.concatenate([x1,x2],axis=-1)\n",
      "        output = self.fc(x3)\n",
      "        return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method DCN.call of <__main__.DCN object at 0x00000183593F28D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method DCN.call of <__main__.DCN object at 0x00000183593F28D0>>, which Python reported as:\n",
      "    def call(self,feat_index,feat_value):\n",
      "        \n",
      "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
      "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
      "#         print(feat_value.get_shape())\n",
      "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
      "        # print(\"feat_embedding:\",feat_embedding.get_shape()) # 32 * 39 * 10\n",
      "        stack_input = tf.keras.layers.Reshape((1,-1))(feat_embedding)\n",
      "        # print(\"stack_input:\",stack_input.get_shape()) # 32 * 1 * 390\n",
      "        \n",
      "        x1 = self.crosslayer(stack_input)\n",
      "        x2 = self.deep(stack_input)\n",
      "        \n",
      "        x3 = tf.keras.layers.concatenate([x1,x2],axis=-1)\n",
      "        output = self.fc(x3)\n",
      "        return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method DCN.call of <__main__.DCN object at 0x00000183593F28D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method DCN.call of <__main__.DCN object at 0x00000183593F28D0>>, which Python reported as:\n",
      "    def call(self,feat_index,feat_value):\n",
      "        \n",
      "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
      "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
      "#         print(feat_value.get_shape())\n",
      "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
      "        # print(\"feat_embedding:\",feat_embedding.get_shape()) # 32 * 39 * 10\n",
      "        stack_input = tf.keras.layers.Reshape((1,-1))(feat_embedding)\n",
      "        # print(\"stack_input:\",stack_input.get_shape()) # 32 * 1 * 390\n",
      "        \n",
      "        x1 = self.crosslayer(stack_input)\n",
      "        x2 = self.deep(stack_input)\n",
      "        \n",
      "        x3 = tf.keras.layers.concatenate([x1,x2],axis=-1)\n",
      "        output = self.fc(x3)\n",
      "        return output\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "Epoch 1, Loss: 0.565358579158783, Accuracy: 0.790395200252533\n",
      "Epoch 2, Loss: 0.5333142280578613, Accuracy: 0.7906453013420105\n",
      "Epoch 3, Loss: 0.5188921093940735, Accuracy: 0.7907286882400513\n",
      "Epoch 4, Loss: 0.5085805654525757, Accuracy: 0.790770411491394\n",
      "Epoch 5, Loss: 0.5001382231712341, Accuracy: 0.7907953858375549\n",
      "Epoch 6, Loss: 0.49196508526802063, Accuracy: 0.790812075138092\n",
      "Epoch 7, Loss: 0.4845847487449646, Accuracy: 0.791538655757904\n",
      "Epoch 8, Loss: 0.4777772128582001, Accuracy: 0.7933967113494873\n",
      "Epoch 9, Loss: 0.4712851643562317, Accuracy: 0.7953976988792419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.46522337198257446, Accuracy: 0.797548770904541\n",
      "Epoch 11, Loss: 0.4593830108642578, Accuracy: 0.799308717250824\n",
      "Epoch 12, Loss: 0.4535185396671295, Accuracy: 0.8014007210731506\n",
      "Epoch 13, Loss: 0.4476926326751709, Accuracy: 0.8034401535987854\n",
      "Epoch 14, Loss: 0.4420176148414612, Accuracy: 0.8057957291603088\n",
      "Epoch 15, Loss: 0.43604835867881775, Accuracy: 0.8078039288520813\n",
      "Epoch 16, Loss: 0.430029958486557, Accuracy: 0.8101238012313843\n",
      "Epoch 17, Loss: 0.4236184060573578, Accuracy: 0.8130241632461548\n",
      "Epoch 18, Loss: 0.41711094975471497, Accuracy: 0.8157690167427063\n",
      "Epoch 19, Loss: 0.410213828086853, Accuracy: 0.8188567757606506\n",
      "Epoch 20, Loss: 0.40275657176971436, Accuracy: 0.8226613402366638\n",
      "Epoch 21, Loss: 0.3947707712650299, Accuracy: 0.8265085220336914\n",
      "Epoch 22, Loss: 0.3864079415798187, Accuracy: 0.8308699727058411\n",
      "Epoch 23, Loss: 0.37755030393600464, Accuracy: 0.8352872133255005\n",
      "Epoch 24, Loss: 0.3682657480239868, Accuracy: 0.8399407863616943\n",
      "Epoch 25, Loss: 0.3589519262313843, Accuracy: 0.8447423577308655\n",
      "Epoch 26, Loss: 0.3493313491344452, Accuracy: 0.8495401740074158\n",
      "Epoch 27, Loss: 0.33972665667533875, Accuracy: 0.8542419075965881\n",
      "Epoch 28, Loss: 0.33029282093048096, Accuracy: 0.8588579893112183\n",
      "Epoch 29, Loss: 0.3210965692996979, Accuracy: 0.8632591962814331\n",
      "Epoch 30, Loss: 0.3121466338634491, Accuracy: 0.8674670457839966\n",
      "Epoch 31, Loss: 0.3034890294075012, Accuracy: 0.8714196085929871\n",
      "Epoch 32, Loss: 0.2950327396392822, Accuracy: 0.8753126859664917\n",
      "Epoch 33, Loss: 0.2869029939174652, Accuracy: 0.8790152668952942\n",
      "Epoch 34, Loss: 0.27917614579200745, Accuracy: 0.8824853897094727\n",
      "Epoch 35, Loss: 0.27175894379615784, Accuracy: 0.8858000636100769\n",
      "Epoch 36, Loss: 0.2646080255508423, Accuracy: 0.8889583945274353\n",
      "Epoch 37, Loss: 0.2577793300151825, Accuracy: 0.8919594883918762\n",
      "Epoch 38, Loss: 0.2512573003768921, Accuracy: 0.8948026895523071\n",
      "Epoch 39, Loss: 0.24505917727947235, Accuracy: 0.897487223148346\n",
      "Epoch 40, Loss: 0.23911045491695404, Accuracy: 0.9000500440597534\n",
      "Epoch 41, Loss: 0.23342998325824738, Accuracy: 0.9024878144264221\n",
      "Epoch 42, Loss: 0.22800736129283905, Accuracy: 0.9048095345497131\n",
      "Epoch 43, Loss: 0.22281348705291748, Accuracy: 0.9070232510566711\n",
      "Epoch 44, Loss: 0.21784667670726776, Accuracy: 0.9091364145278931\n",
      "Epoch 45, Loss: 0.2130877673625946, Accuracy: 0.9111555814743042\n",
      "Epoch 46, Loss: 0.20853079855442047, Accuracy: 0.9130869507789612\n",
      "Epoch 47, Loss: 0.2041596919298172, Accuracy: 0.9149361848831177\n",
      "Epoch 48, Loss: 0.19996346533298492, Accuracy: 0.9167083501815796\n",
      "Epoch 49, Loss: 0.19593490660190582, Accuracy: 0.9184081554412842\n",
      "Epoch 50, Loss: 0.19206039607524872, Accuracy: 0.9200400114059448\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    for label, idx, value in train_ds:\n",
    "        train_one_step(dcn,optimizer,idx, value,label)\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
    "    print (template.format(epoch+1,\n",
    "                             train_loss.result(),train_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
