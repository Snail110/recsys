{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\julianxu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn.preprocessing import OneHotEncoder,StandarScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNN(tf.keras.Model):\n",
    "    def __init__(self,num_feat,num_field,dropout_deep,deep_layer_sizes,product_layer_dim=10,reg_l1=0.01,reg_l2=1e-5,embedding_size=10,product_type='outer'):\n",
    "        super().__init__()\n",
    "        self.reg_l1 = reg_l1\n",
    "        self.reg_l2 = reg_l2\n",
    "        self.num_feat = num_feat # F =features nums\n",
    "        self.num_field = num_field # N =fields of a feature \n",
    "        self.product_layer_dim = product_layer_dim # D1 pnn dim\n",
    "        self.dropout_deep  = dropout_deep\n",
    "        \n",
    "        # Embedding 这里采用embeddings层因此大小为F* M F为特征数量，M为embedding的维度\n",
    "        feat_embeddings = tf.keras.layers.Embedding(num_feat, embedding_size, embeddings_initializer='uniform') # F * M \n",
    "        self.feat_embeddings = feat_embeddings\n",
    "        \n",
    "        # 定义随机初始化\n",
    "        initializer = tf.initializers.GlorotUniform()\n",
    "        \n",
    "        # linear part 线性层就是embedding层的复制，因此线性信号权重大小是D1 * N * M，为什么因此是线性层维度为 D1，embedding层维度为N* M\n",
    "        # 因此权重大小为D1 * N *M\n",
    "        self.linear_weights = tf.Variable(initializer(shape=(product_layer_dim,num_field,embedding_size))) # D1 * N * M\n",
    "        \n",
    "        # quadratic part \n",
    "        self.product_type = product_type\n",
    "        if product_type == 'inner':\n",
    "            self.theta = tf.Variable(initializer(shape=(product_layer_dim,num_field))) # D1 * N\n",
    "\n",
    "        else:\n",
    "            self.quadratic_weights = tf.Variable(initializer(shape=(product_layer_dim,embedding_size, embedding_size)))# D1 * M * M\n",
    "        \n",
    "        # fc layer\n",
    "        self.deep_layer_sizes = deep_layer_sizes\n",
    "        #神经网络方面的参数\n",
    "        for i in range(len(deep_layer_sizes)):\n",
    "            setattr(self, 'dense_' + str(i),tf.keras.layers.Dense(deep_layer_sizes[i]))\n",
    "            setattr(self, 'batchNorm_' + str(i),tf.keras.layers.BatchNormalization())\n",
    "            setattr(self, 'activation_' + str(i),tf.keras.layers.Activation('relu'))\n",
    "            setattr(self, 'dropout_' + str(i),tf.keras.layers.Dropout(dropout_deep[i]))\n",
    "        \n",
    "        # last layer\n",
    "        self.fc = tf.keras.layers.Dense(1,activation=None,use_bias=True)\n",
    "        \n",
    "    def call(self,feat_index,feat_value):\n",
    "        # call函数接收输入变量\n",
    "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
    "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
    "#         print(feat_value.get_shape())\n",
    "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
    "        # linear part \n",
    "        lz = tf.einsum('bnm,dnm->bd',feat_embedding,self.linear_weights) # Batch * D1\n",
    "        \n",
    "        # quadratic part\n",
    "        if self.product_type == 'inner':\n",
    "            theta = tf.einsum('bnm,dn->bdnm',feat_embedding,self.theta) # Batch * D1 * N * M \n",
    "            lp = tf.einsum('bdnm,bdnm->bd',theta,theta) # Batch * D1\n",
    "        else:\n",
    "            embed_sum = tf.reduce_sum(feat_embedding,axis=1) # Batch * M\n",
    "            p = tf.einsum('bm,bn->bmn',embed_sum,embed_sum)\n",
    "            lp = tf.einsum('bmn,dmn->bd',p,self.quadratic_weights) # Batch * D1\n",
    "        \n",
    "        y_deep = tf.concat((lz,lp),axis=1)\n",
    "        y_deep = tf.keras.layers.Dropout(self.dropout_deep[0])(y_deep)\n",
    "        \n",
    "        for i in range(len(self.deep_layer_sizes)):\n",
    "            y_deep = getattr(self,'dense_' + str(i))(y_deep)\n",
    "            y_deep = getattr(self,'batchNorm_' + str(i))(y_deep)\n",
    "            y_deep = getattr(self,'activation_' + str(i))(y_deep)\n",
    "            y_deep = getattr(self,'dropout_' + str(i))(y_deep)\n",
    "        \n",
    "        output = self.fc(y_deep)\n",
    "        \n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'F:\\baidudownload\\kaggle-2014-criteo-master\\kaggle-2014-criteo-master\\train.tiny.csv')\n",
    "\n",
    "train = train.fillna(0)\n",
    "\n",
    "traindrop = train.drop(columns = ['Id'])\n",
    "\n",
    "traindrop.to_csv(r'F:\\baidudownload\\kaggle-2014-criteo-master\\kaggle-2014-criteo-master\\train.txt',sep='\\t', index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_ = 10\n",
    "# dir_feat_dict_ = 'feat_dict_' + str(freq_) + '.pkl2'\n",
    "continuous_range_ = range(1, 14)\n",
    "categorical_range_ = range(14, 40)\n",
    "\n",
    "# 统计离散特征每个离散值出现的次数组成字典\n",
    "feat_cnt = Counter()\n",
    "with open(r'F:\\baidudownload\\kaggle-2014-criteo-master\\kaggle-2014-criteo-master\\train.txt', 'r') as fin:\n",
    "    for line_idx, line in enumerate(fin):\n",
    "        features = line.rstrip('\\n').split('\\t')\n",
    "        for idx in categorical_range_:\n",
    "            if features[idx] == '': continue\n",
    "            feat_cnt.update([features[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only retain discrete features with high frequency\n",
    "dis_feat_set = set() # 高频段的离散字符\n",
    "for feat, ot in feat_cnt.items():\n",
    "    if ot >= freq_:\n",
    "        dis_feat_set.add(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for continuous and discrete features\n",
    "feat_dict = {}\n",
    "tc = 1\n",
    "# Continuous features\n",
    "for idx in continuous_range_:\n",
    "    feat_dict[idx] = tc\n",
    "    tc += 1 # 代表占据一列\n",
    "\n",
    "# Discrete features\n",
    "cnt_feat_set = set()\n",
    "with open(r'F:\\baidudownload\\kaggle-2014-criteo-master\\kaggle-2014-criteo-master\\train.txt', 'r') as fin:\n",
    "    for line_idx, line in enumerate(fin):\n",
    "        features = line.rstrip('\\n').split('\\t')\n",
    "        for idx in categorical_range_:\n",
    "            # 排除空字符和低频离散字符\n",
    "            if features[idx] == '' or features[idx] not in dis_feat_set:\n",
    "                continue\n",
    "            # 排除连续性数值\n",
    "            if features[idx] not in cnt_feat_set:\n",
    "                cnt_feat_set.add(features[idx])\n",
    "                # 获取种类数\n",
    "                feat_dict[features[idx]] = tc\n",
    "                tc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"F:\\\\baidudownload\\\\kaggle-2014-criteo-master\\\\kaggle-2014-criteo-master\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features=['I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9',\n",
    "       'I10', 'I11', 'I12', 'I13']\n",
    "dist_features = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7',\n",
    "       'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17',\n",
    "       'C18', 'C19', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = []\n",
    "train_value = []\n",
    "train_idx = []\n",
    "test_label = []\n",
    "test_value = []\n",
    "test_idx = []\n",
    "\n",
    "continuous_range_ = range(1, 14)\n",
    "categorical_range_ = range(14, 40)\n",
    "cont_max_=[]\n",
    "cont_min_=[]\n",
    "for cf in cont_features:\n",
    "    cont_max_.append(max(train[cf]))\n",
    "    cont_min_.append(min(train[cf]))\n",
    "cont_diff_ = [cont_max_[i] - cont_min_[i] for i in range(len(cont_min_))]\n",
    "\n",
    "def process_line_(line):\n",
    "    features = line.rstrip('\\n').split('\\t')\n",
    "    feat_idx, feat_value, label = [], [], []\n",
    "\n",
    "    # MinMax Normalization\n",
    "    for idx in continuous_range_:\n",
    "        if features[idx] == '':\n",
    "            feat_idx.append(0)\n",
    "            feat_value.append(0.0)\n",
    "        else:\n",
    "            feat_idx.append(feat_dict[idx])\n",
    "            # 归一化\n",
    "            feat_value.append(round((float(features[idx]) - cont_min_[idx - 1]) / cont_diff_[idx - 1], 6))\n",
    "\n",
    "    # 处理离散型数据\n",
    "    for idx in categorical_range_:\n",
    "        if features[idx] == '' or features[idx] not in feat_dict:\n",
    "            feat_idx.append(0)\n",
    "            feat_value.append(0.0)\n",
    "        else:\n",
    "            feat_idx.append(feat_dict[features[idx]])\n",
    "            feat_value.append(1.0)\n",
    "    return feat_idx, feat_value, [int(features[0])]\n",
    "split_ratio = 0.9\n",
    "with open(file_path + 'train.txt', 'r') as fin:\n",
    "    for line_idx, line in enumerate(fin):\n",
    "\n",
    "        feat_idx, feat_value, label = process_line_(line)\n",
    "        if np.random.random() <= split_ratio:\n",
    "            train_label.append(label)\n",
    "            train_idx.append(feat_idx)\n",
    "            train_value.append(feat_value)\n",
    "        else:\n",
    "            test_label.append(label)\n",
    "            test_idx.append(feat_idx)\n",
    "            test_value.append(feat_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnn = PNN(num_feat=len(feat_dict) + 1, num_field=39, dropout_deep=[0.5, 0.5, 0.5],\n",
    "                deep_layer_sizes=[400, 400], product_layer_dim=10,\n",
    "                reg_l1=0.01, reg_l2=1e-5, embedding_size=10, product_type='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_label,train_idx,train_value)).shuffle(10000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_one_step(model, optimizer, idx, value, label):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(idx,value)\n",
    "        loss = loss_object(y_true=label, y_pred=output)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    grads = [tf.clip_by_norm(g, 100) for g in grads]\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(label,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_acc')\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method PNN.call of <__main__.PNN object at 0x0000022281C438D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method PNN.call of <__main__.PNN object at 0x0000022281C438D0>>, which Python reported as:\n",
      "    def call(self,feat_index,feat_value):\n",
      "        # call函数接收输入变量\n",
      "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
      "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
      "#         print(feat_value.get_shape())\n",
      "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
      "        # linear part \n",
      "        lz = tf.einsum('bnm,dnm->bd',feat_embedding,self.linear_weights) # Batch * D1\n",
      "        \n",
      "        # quadratic part\n",
      "        if self.product_type == 'inner':\n",
      "            theta = tf.einsum('bnm,dn->bdnm',feat_embedding,self.theta) # Batch * D1 * N * M \n",
      "            lp = tf.einsum('bdnm,bdnm->bd',theta,theta) # Batch * D1\n",
      "        else:\n",
      "            embed_sum = tf.reduce_sum(feat_embedding,axis=1) # Batch * M\n",
      "            p = tf.einsum('bm,bn->bmn',embed_sum,embed_sum)\n",
      "            lp = tf.einsum('bmn,dmn->bd',p,self.quadratic_weights) # Batch * D1\n",
      "        \n",
      "        y_deep = tf.concat((lz,lp),axis=1)\n",
      "        y_deep = tf.keras.layers.Dropout(self.dropout_deep[0])(y_deep)\n",
      "        \n",
      "        for i in range(len(self.deep_layer_sizes)):\n",
      "            y_deep = getattr(self,'dense_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'batchNorm_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'activation_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'dropout_' + str(i))(y_deep)\n",
      "        \n",
      "        output = self.fc(y_deep)\n",
      "        \n",
      "        return output \n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method PNN.call of <__main__.PNN object at 0x0000022281C438D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method PNN.call of <__main__.PNN object at 0x0000022281C438D0>>, which Python reported as:\n",
      "    def call(self,feat_index,feat_value):\n",
      "        # call函数接收输入变量\n",
      "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
      "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
      "#         print(feat_value.get_shape())\n",
      "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
      "        # linear part \n",
      "        lz = tf.einsum('bnm,dnm->bd',feat_embedding,self.linear_weights) # Batch * D1\n",
      "        \n",
      "        # quadratic part\n",
      "        if self.product_type == 'inner':\n",
      "            theta = tf.einsum('bnm,dn->bdnm',feat_embedding,self.theta) # Batch * D1 * N * M \n",
      "            lp = tf.einsum('bdnm,bdnm->bd',theta,theta) # Batch * D1\n",
      "        else:\n",
      "            embed_sum = tf.reduce_sum(feat_embedding,axis=1) # Batch * M\n",
      "            p = tf.einsum('bm,bn->bmn',embed_sum,embed_sum)\n",
      "            lp = tf.einsum('bmn,dmn->bd',p,self.quadratic_weights) # Batch * D1\n",
      "        \n",
      "        y_deep = tf.concat((lz,lp),axis=1)\n",
      "        y_deep = tf.keras.layers.Dropout(self.dropout_deep[0])(y_deep)\n",
      "        \n",
      "        for i in range(len(self.deep_layer_sizes)):\n",
      "            y_deep = getattr(self,'dense_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'batchNorm_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'activation_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'dropout_' + str(i))(y_deep)\n",
      "        \n",
      "        output = self.fc(y_deep)\n",
      "        \n",
      "        return output \n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method PNN.call of <__main__.PNN object at 0x0000022281C438D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method PNN.call of <__main__.PNN object at 0x0000022281C438D0>>, which Python reported as:\n",
      "    def call(self,feat_index,feat_value):\n",
      "        # call函数接收输入变量\n",
      "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
      "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
      "#         print(feat_value.get_shape())\n",
      "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
      "        # linear part \n",
      "        lz = tf.einsum('bnm,dnm->bd',feat_embedding,self.linear_weights) # Batch * D1\n",
      "        \n",
      "        # quadratic part\n",
      "        if self.product_type == 'inner':\n",
      "            theta = tf.einsum('bnm,dn->bdnm',feat_embedding,self.theta) # Batch * D1 * N * M \n",
      "            lp = tf.einsum('bdnm,bdnm->bd',theta,theta) # Batch * D1\n",
      "        else:\n",
      "            embed_sum = tf.reduce_sum(feat_embedding,axis=1) # Batch * M\n",
      "            p = tf.einsum('bm,bn->bmn',embed_sum,embed_sum)\n",
      "            lp = tf.einsum('bmn,dmn->bd',p,self.quadratic_weights) # Batch * D1\n",
      "        \n",
      "        y_deep = tf.concat((lz,lp),axis=1)\n",
      "        y_deep = tf.keras.layers.Dropout(self.dropout_deep[0])(y_deep)\n",
      "        \n",
      "        for i in range(len(self.deep_layer_sizes)):\n",
      "            y_deep = getattr(self,'dense_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'batchNorm_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'activation_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'dropout_' + str(i))(y_deep)\n",
      "        \n",
      "        output = self.fc(y_deep)\n",
      "        \n",
      "        return output \n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method PNN.call of <__main__.PNN object at 0x0000022281C438D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method PNN.call of <__main__.PNN object at 0x0000022281C438D0>>, which Python reported as:\n",
      "    def call(self,feat_index,feat_value):\n",
      "        # call函数接收输入变量\n",
      "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
      "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
      "#         print(feat_value.get_shape())\n",
      "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
      "        # linear part \n",
      "        lz = tf.einsum('bnm,dnm->bd',feat_embedding,self.linear_weights) # Batch * D1\n",
      "        \n",
      "        # quadratic part\n",
      "        if self.product_type == 'inner':\n",
      "            theta = tf.einsum('bnm,dn->bdnm',feat_embedding,self.theta) # Batch * D1 * N * M \n",
      "            lp = tf.einsum('bdnm,bdnm->bd',theta,theta) # Batch * D1\n",
      "        else:\n",
      "            embed_sum = tf.reduce_sum(feat_embedding,axis=1) # Batch * M\n",
      "            p = tf.einsum('bm,bn->bmn',embed_sum,embed_sum)\n",
      "            lp = tf.einsum('bmn,dmn->bd',p,self.quadratic_weights) # Batch * D1\n",
      "        \n",
      "        y_deep = tf.concat((lz,lp),axis=1)\n",
      "        y_deep = tf.keras.layers.Dropout(self.dropout_deep[0])(y_deep)\n",
      "        \n",
      "        for i in range(len(self.deep_layer_sizes)):\n",
      "            y_deep = getattr(self,'dense_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'batchNorm_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'activation_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'dropout_' + str(i))(y_deep)\n",
      "        \n",
      "        output = self.fc(y_deep)\n",
      "        \n",
      "        return output \n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method PNN.call of <__main__.PNN object at 0x0000022281C438D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method PNN.call of <__main__.PNN object at 0x0000022281C438D0>>, which Python reported as:\n",
      "    def call(self,feat_index,feat_value):\n",
      "        # call函数接收输入变量\n",
      "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
      "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
      "#         print(feat_value.get_shape())\n",
      "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
      "        # linear part \n",
      "        lz = tf.einsum('bnm,dnm->bd',feat_embedding,self.linear_weights) # Batch * D1\n",
      "        \n",
      "        # quadratic part\n",
      "        if self.product_type == 'inner':\n",
      "            theta = tf.einsum('bnm,dn->bdnm',feat_embedding,self.theta) # Batch * D1 * N * M \n",
      "            lp = tf.einsum('bdnm,bdnm->bd',theta,theta) # Batch * D1\n",
      "        else:\n",
      "            embed_sum = tf.reduce_sum(feat_embedding,axis=1) # Batch * M\n",
      "            p = tf.einsum('bm,bn->bmn',embed_sum,embed_sum)\n",
      "            lp = tf.einsum('bmn,dmn->bd',p,self.quadratic_weights) # Batch * D1\n",
      "        \n",
      "        y_deep = tf.concat((lz,lp),axis=1)\n",
      "        y_deep = tf.keras.layers.Dropout(self.dropout_deep[0])(y_deep)\n",
      "        \n",
      "        for i in range(len(self.deep_layer_sizes)):\n",
      "            y_deep = getattr(self,'dense_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'batchNorm_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'activation_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'dropout_' + str(i))(y_deep)\n",
      "        \n",
      "        output = self.fc(y_deep)\n",
      "        \n",
      "        return output \n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method PNN.call of <__main__.PNN object at 0x0000022281C438D0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method PNN.call of <__main__.PNN object at 0x0000022281C438D0>>, which Python reported as:\n",
      "    def call(self,feat_index,feat_value):\n",
      "        # call函数接收输入变量\n",
      "        # embedding part  feat_index = inputs为输入 feat_embeddings为一个layer。\n",
      "        feat_embedding_0 = self.feat_embeddings(feat_index) # Batch * N * M \n",
      "#         print(feat_value.get_shape())\n",
      "        feat_embedding = tf.einsum('bnm,bn->bnm',feat_embedding_0,feat_value)\n",
      "        # linear part \n",
      "        lz = tf.einsum('bnm,dnm->bd',feat_embedding,self.linear_weights) # Batch * D1\n",
      "        \n",
      "        # quadratic part\n",
      "        if self.product_type == 'inner':\n",
      "            theta = tf.einsum('bnm,dn->bdnm',feat_embedding,self.theta) # Batch * D1 * N * M \n",
      "            lp = tf.einsum('bdnm,bdnm->bd',theta,theta) # Batch * D1\n",
      "        else:\n",
      "            embed_sum = tf.reduce_sum(feat_embedding,axis=1) # Batch * M\n",
      "            p = tf.einsum('bm,bn->bmn',embed_sum,embed_sum)\n",
      "            lp = tf.einsum('bmn,dmn->bd',p,self.quadratic_weights) # Batch * D1\n",
      "        \n",
      "        y_deep = tf.concat((lz,lp),axis=1)\n",
      "        y_deep = tf.keras.layers.Dropout(self.dropout_deep[0])(y_deep)\n",
      "        \n",
      "        for i in range(len(self.deep_layer_sizes)):\n",
      "            y_deep = getattr(self,'dense_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'batchNorm_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'activation_' + str(i))(y_deep)\n",
      "            y_deep = getattr(self,'dropout_' + str(i))(y_deep)\n",
      "        \n",
      "        output = self.fc(y_deep)\n",
      "        \n",
      "        return output \n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "Epoch 1, Loss: 0.6576068997383118, Accuracy: 0.7935805320739746\n",
      "Epoch 2, Loss: 0.5885103344917297, Accuracy: 0.7927504181861877\n",
      "Epoch 3, Loss: 0.5613061785697937, Accuracy: 0.7932115793228149\n",
      "Epoch 4, Loss: 0.5463097095489502, Accuracy: 0.7933038473129272\n",
      "Epoch 5, Loss: 0.5362721681594849, Accuracy: 0.7933591604232788\n",
      "Epoch 6, Loss: 0.5272665023803711, Accuracy: 0.7933960556983948\n",
      "Epoch 7, Loss: 0.519040048122406, Accuracy: 0.7934224009513855\n",
      "Epoch 8, Loss: 0.5109833478927612, Accuracy: 0.7934421896934509\n",
      "Epoch 9, Loss: 0.5033180713653564, Accuracy: 0.7937650084495544\n",
      "Epoch 10, Loss: 0.4961019456386566, Accuracy: 0.7946319580078125\n",
      "Epoch 11, Loss: 0.4890766143798828, Accuracy: 0.7957438230514526\n",
      "Epoch 12, Loss: 0.4827176034450531, Accuracy: 0.7968548536300659\n",
      "Epoch 13, Loss: 0.47674980759620667, Accuracy: 0.797752320766449\n",
      "Epoch 14, Loss: 0.4711345434188843, Accuracy: 0.7988378405570984\n",
      "Epoch 15, Loss: 0.4657707214355469, Accuracy: 0.800000011920929\n",
      "Epoch 16, Loss: 0.46048682928085327, Accuracy: 0.8010168671607971\n",
      "Epoch 17, Loss: 0.45555245876312256, Accuracy: 0.8017839193344116\n",
      "Epoch 18, Loss: 0.45082414150238037, Accuracy: 0.8027116656303406\n",
      "Epoch 19, Loss: 0.44618847966194153, Accuracy: 0.8038330674171448\n",
      "Epoch 20, Loss: 0.4417554438114166, Accuracy: 0.8047869205474854\n",
      "Epoch 21, Loss: 0.4372897744178772, Accuracy: 0.8057553768157959\n",
      "Epoch 22, Loss: 0.4330126941204071, Accuracy: 0.8065603375434875\n",
      "Epoch 23, Loss: 0.4288385808467865, Accuracy: 0.8073434233665466\n",
      "Epoch 24, Loss: 0.4247806966304779, Accuracy: 0.8084993362426758\n",
      "Epoch 25, Loss: 0.4208265244960785, Accuracy: 0.8095406889915466\n",
      "Epoch 26, Loss: 0.4169451892375946, Accuracy: 0.8103954792022705\n",
      "Epoch 27, Loss: 0.4130288362503052, Accuracy: 0.8114328980445862\n",
      "Epoch 28, Loss: 0.40920114517211914, Accuracy: 0.8125543594360352\n",
      "Epoch 29, Loss: 0.40538835525512695, Accuracy: 0.8135412335395813\n",
      "Epoch 30, Loss: 0.4015306830406189, Accuracy: 0.8148680925369263\n",
      "Epoch 31, Loss: 0.3976829946041107, Accuracy: 0.8161808252334595\n",
      "Epoch 32, Loss: 0.3940720558166504, Accuracy: 0.8173941373825073\n",
      "Epoch 33, Loss: 0.3902757167816162, Accuracy: 0.8186178207397461\n",
      "Epoch 34, Loss: 0.3865205645561218, Accuracy: 0.8198021054267883\n",
      "Epoch 35, Loss: 0.3826711177825928, Accuracy: 0.8213613629341125\n",
      "Epoch 36, Loss: 0.3788437247276306, Accuracy: 0.8228186964988708\n",
      "Epoch 37, Loss: 0.37512916326522827, Accuracy: 0.8244065642356873\n",
      "Epoch 38, Loss: 0.37138548493385315, Accuracy: 0.8259109258651733\n",
      "Epoch 39, Loss: 0.3679004907608032, Accuracy: 0.8273097276687622\n",
      "Epoch 40, Loss: 0.36420953273773193, Accuracy: 0.8287769556045532\n",
      "Epoch 41, Loss: 0.3605599105358124, Accuracy: 0.8302671313285828\n",
      "Epoch 42, Loss: 0.3569217622280121, Accuracy: 0.8318312168121338\n",
      "Epoch 43, Loss: 0.3537658751010895, Accuracy: 0.8332582712173462\n",
      "Epoch 44, Loss: 0.35020676255226135, Accuracy: 0.8348342180252075\n",
      "Epoch 45, Loss: 0.3465138077735901, Accuracy: 0.8364631533622742\n",
      "Epoch 46, Loss: 0.34285783767700195, Accuracy: 0.8380813598632812\n",
      "Epoch 47, Loss: 0.3392927646636963, Accuracy: 0.8396543264389038\n",
      "Epoch 48, Loss: 0.33572396636009216, Accuracy: 0.8413115739822388\n",
      "Epoch 49, Loss: 0.33254799246788025, Accuracy: 0.8427882790565491\n",
      "Epoch 50, Loss: 0.3292645514011383, Accuracy: 0.8442501425743103\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    for label, idx, value in train_ds:\n",
    "        train_one_step(pnn,optimizer,idx, value,label)\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
    "    print (template.format(epoch+1,\n",
    "                             train_loss.result(),train_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
